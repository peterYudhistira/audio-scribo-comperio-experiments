{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>IMPORTS</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports go here\n",
    "import numpy as np\n",
    "import db\n",
    "import inflect\n",
    "import string\n",
    "import nltk\n",
    "import gensim\n",
    "import contractions\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim.downloader as api\n",
    "from gensim import corpora, models\n",
    "from nltk.test.gensim_fixt import setup_module\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Init (don't forget to do this when making the class, and only once.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_module()\n",
    "model = api.load(\"glove-wiki-gigaword-300\")\n",
    "\n",
    "# see here : https://radimrehurek.com/gensim/downloader.html for saving."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Retrieval</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>event_title</th>\n",
       "      <th>speaker</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>230</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>KSMG</td>\n",
       "      <td>Pro life or pro choice?</td>\n",
       "      <td>I am pro choice. I feel bad for the baby. Peop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>231</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>JJ</td>\n",
       "      <td>Pro life or pro choice?</td>\n",
       "      <td>Logically, both make sense. Conflicted between...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>232</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>JER</td>\n",
       "      <td>Pro life or pro choice?</td>\n",
       "      <td>I'm pro life. Because in my belief, if a fetus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>233</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>YOT</td>\n",
       "      <td>Pro life or pro choice?</td>\n",
       "      <td>Prochoice. If she was a victim of rape, etc, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>234</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>GRE</td>\n",
       "      <td>Pro life or pro choice?</td>\n",
       "      <td>Prolife for religious reasons. Being religious...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>235</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>RIC</td>\n",
       "      <td>Pro life or pro choice?</td>\n",
       "      <td>Prochoice. Unless the baby is normal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>236</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>GRE</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>Legal - not really legal - legal for special c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>237</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>MAR</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>Agree with other solutions besides abortion - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>238</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>RIC</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>Should be legal with criteria. Agree with Indo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>239</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>YOR</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>What counts as a person? Is fetus a person?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>240</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>GRE</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>Does not being a Christian still fall under Ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>241</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>YOT</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>If it has a heartbeat, can it be called life? ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>242</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>JJ</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>when something grows doesnt it count as life a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>243</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>STN</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>KBBI - the process of baby child teenager adul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>244</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>YOT</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>I don't agree with Adam and Eve because not ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>245</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>STN</td>\n",
       "      <td>We see from our side that maybe they are miser...</td>\n",
       "      <td>Everyone has rights, even fetuses or babies, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>246</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>GRE</td>\n",
       "      <td>We see from our side that maybe they are miser...</td>\n",
       "      <td>Is it wrong to regret life? That is the right ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>247</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>YOT</td>\n",
       "      <td>We see from our side that maybe they are miser...</td>\n",
       "      <td>Actually, we humans are well aware that life i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>248</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>STN</td>\n",
       "      <td>We see from our side that maybe they are miser...</td>\n",
       "      <td>The law didn't need to be revised. Choosing th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>249</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>YOT</td>\n",
       "      <td>We see from our side that maybe they are miser...</td>\n",
       "      <td>Rape victims, let's not confuse them, let's di...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id         event_title speaker  \\\n",
       "0   230  Of Choice and Life    KSMG   \n",
       "1   231  Of Choice and Life      JJ   \n",
       "2   232  Of Choice and Life     JER   \n",
       "3   233  Of Choice and Life     YOT   \n",
       "4   234  Of Choice and Life     GRE   \n",
       "5   235  Of Choice and Life     RIC   \n",
       "6   236  Of Choice and Life     GRE   \n",
       "7   237  Of Choice and Life     MAR   \n",
       "8   238  Of Choice and Life     RIC   \n",
       "9   239  Of Choice and Life     YOR   \n",
       "10  240  Of Choice and Life     GRE   \n",
       "11  241  Of Choice and Life     YOT   \n",
       "12  242  Of Choice and Life      JJ   \n",
       "13  243  Of Choice and Life     STN   \n",
       "14  244  Of Choice and Life     YOT   \n",
       "15  245  Of Choice and Life     STN   \n",
       "16  246  Of Choice and Life     GRE   \n",
       "17  247  Of Choice and Life     YOT   \n",
       "18  248  Of Choice and Life     STN   \n",
       "19  249  Of Choice and Life     YOT   \n",
       "\n",
       "                                             question  \\\n",
       "0                             Pro life or pro choice?   \n",
       "1                             Pro life or pro choice?   \n",
       "2                             Pro life or pro choice?   \n",
       "3                             Pro life or pro choice?   \n",
       "4                             Pro life or pro choice?   \n",
       "5                             Pro life or pro choice?   \n",
       "6              Do you think abortion should be legal?   \n",
       "7              Do you think abortion should be legal?   \n",
       "8              Do you think abortion should be legal?   \n",
       "9              Do you think abortion should be legal?   \n",
       "10             Do you think abortion should be legal?   \n",
       "11             Do you think abortion should be legal?   \n",
       "12             Do you think abortion should be legal?   \n",
       "13             Do you think abortion should be legal?   \n",
       "14             Do you think abortion should be legal?   \n",
       "15  We see from our side that maybe they are miser...   \n",
       "16  We see from our side that maybe they are miser...   \n",
       "17  We see from our side that maybe they are miser...   \n",
       "18  We see from our side that maybe they are miser...   \n",
       "19  We see from our side that maybe they are miser...   \n",
       "\n",
       "                                               answer  \n",
       "0   I am pro choice. I feel bad for the baby. Peop...  \n",
       "1   Logically, both make sense. Conflicted between...  \n",
       "2   I'm pro life. Because in my belief, if a fetus...  \n",
       "3   Prochoice. If she was a victim of rape, etc, s...  \n",
       "4   Prolife for religious reasons. Being religious...  \n",
       "5               Prochoice. Unless the baby is normal.  \n",
       "6   Legal - not really legal - legal for special c...  \n",
       "7   Agree with other solutions besides abortion - ...  \n",
       "8   Should be legal with criteria. Agree with Indo...  \n",
       "9         What counts as a person? Is fetus a person?  \n",
       "10  Does not being a Christian still fall under Ch...  \n",
       "11  If it has a heartbeat, can it be called life? ...  \n",
       "12  when something grows doesnt it count as life a...  \n",
       "13  KBBI - the process of baby child teenager adul...  \n",
       "14  I don't agree with Adam and Eve because not ev...  \n",
       "15  Everyone has rights, even fetuses or babies, b...  \n",
       "16  Is it wrong to regret life? That is the right ...  \n",
       "17  Actually, we humans are well aware that life i...  \n",
       "18  The law didn't need to be revised. Choosing th...  \n",
       "19  Rape victims, let's not confuse them, let's di...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input : DatabaseHandler\n",
    "# output : DataFrame\n",
    "def GetDF(dh:db.DatabaseHandler, selector: str, eventID: int, splitBySentences: bool = False):\n",
    "    df = dh.get_recordDataJoinedDF(selector=selector, ID=eventID)\n",
    "    if splitBySentences:\n",
    "        # df.set_index('id', inplace=True)\n",
    "        df['answer'] = df['answer'].str.split('.')\n",
    "        df = df.explode(\"answer\", True)\n",
    "        df.drop(df[df[\"answer\"] == \"\"].index, inplace=True)\n",
    "        df.reset_index(drop=True,inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "dh = db.DatabaseHandler(\"testdb.db\")  # db connection\n",
    "df = GetDF(dh, \"event_id\", 20, False)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Preprocessing</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input : sentence/document (string); parameters\n",
    "# output : a list of word tokens (list<string>)\n",
    "def PreprocessDocument(doc:str, isLemma:bool=False, isStopWords:bool=False, isInflect:bool=False, isNumberFiltered:bool=True):\n",
    "    inflector = inflect.engine()\n",
    "    stopwordSet = set(stopwords.words(\"english\"))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    punctuations = string.punctuation\n",
    "    # if numbers are filtered, add that to the punctuation string\n",
    "    if isNumberFiltered:\n",
    "        punctuations += \"1234567890\"\n",
    "\n",
    "    # case fold\n",
    "    doc = doc.lower()\n",
    "\n",
    "    # remove puncs\n",
    "    doc = \"\".join([char for char in doc if char not in punctuations])\n",
    "\n",
    "    # tokenize it.\n",
    "    token_list = nltk.word_tokenize(doc)\n",
    "\n",
    "    for i in range(len(token_list)):\n",
    "        # if inflect\n",
    "        if isInflect:\n",
    "            if token_list[i].isdigit():\n",
    "                token_list[i] = inflector.number_to_words(token_list[i])\n",
    "\n",
    "        # if lemma\n",
    "        if isLemma:\n",
    "            tagged_word = nltk.pos_tag([token_list[i]])\n",
    "            wordnet_pos = getWordnetPos(tagged_word[0][1])\n",
    "            token_list[i] = lemmatizer.lemmatize(tagged_word[0][0], pos=wordnet_pos)\n",
    "        \n",
    "        # if stopword\n",
    "        if isStopWords:\n",
    "            if token_list[i] in stopwordSet or token_list[i].isdigit():\n",
    "                token_list[i] = \"#\" # mark as #\n",
    "        \n",
    "    # remove the marked strings\n",
    "    token_list = [token for token in token_list if token != \"#\"]\n",
    "\n",
    "    if token_list:\n",
    "        return token_list\n",
    "    return [\"\"]\n",
    "\n",
    "def getWordnetPos(tag):\n",
    "    \"\"\"Map POS tag to WordNet POS tag\"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN # solves as noun by default.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['christian', 'still', 'fall', 'christian', 'rule']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mySentence = df.loc[10][\"answer\"]\n",
    "myTokenizedSentence = PreprocessDocument(mySentence, isStopWords=True, isLemma=True)\n",
    "myTokenizedSentence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Get word set, whatever for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288\n"
     ]
    }
   ],
   "source": [
    "def get_word_set(df):\n",
    "    bigtext = \"\"\n",
    "    # join in lower case\n",
    "    for i in range(len(df)):\n",
    "        bigtext += \" {}\".format(df[i].lower())\n",
    "    bigtext = contractions.fix(bigtext) # remove contractions\n",
    "    bigtext = \"\".join([char for char in bigtext if char not in string.punctuation]) # remove punctuations\n",
    "    big_text_tokens = PreprocessDocument(bigtext, isLemma=True) # put in blender like dick\n",
    "    return set(big_text_tokens) # return as set\n",
    "\n",
    "myWordSet = get_word_set(df[\"answer\"])\n",
    "print(len(myWordSet))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> TF-IDF function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input : list<str>\n",
    "# output : Dataframe, matrix\n",
    "def GetTFIDF(doclist:list, isPreprocessed=True):\n",
    "    if not isPreprocessed:\n",
    "        doclist = [PreprocessDocument(doc, isLemma=True, isStopWords=True) for doc in doclist]\n",
    "    else:\n",
    "        # just tokenize the thing\n",
    "        doclist = [nltk.word_tokenize(doc) for doc in doclist]\n",
    "        \n",
    "    flat_doclist = [' '.join(doc) for doc in doclist] # turn into one big corpus\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    matrix  = vectorizer.fit_transform(flat_doclist)\n",
    "    tfidf_keys = vectorizer.get_feature_names_out()\n",
    "    df_tfidf = db.pd.DataFrame(matrix.toarray(), columns=tfidf_keys)\n",
    "\n",
    "    return df_tfidf, matrix\n",
    "\n",
    "\n",
    "def GetTFIDF_Gensim(doclist:list, isPreprocessed=True):\n",
    "    if not isPreprocessed:\n",
    "        doclist = [PreprocessDocument(doc, isLemma=True, isStopWords=True, isInflect=True) for doc in doclist]\n",
    "    else:\n",
    "        doclist = [nltk.word_tokenize(doc) for doc in doclist]\n",
    "    dictionary = corpora.Dictionary(doclist)\n",
    "\n",
    "    return dictionary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tfidf, my_matrix = GetTFIDF(df[\"answer\"], isPreprocessed=True)\n",
    "my_tfidf\n",
    "\n",
    "goofy = GetTFIDF_Gensim(df[\"answer\"], False)\n",
    "len(goofy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Word- and Sentence-Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input : list<str> : tokens of one document/sentence\n",
    "# output : list<(str, list<int>[300])> : list of word-vector pair for each word available on the model\n",
    "def WordEmbed(document: list, model):\n",
    "    word_embed_pairs = []\n",
    "    for word in document:\n",
    "        if word in model:\n",
    "            word_embed_pairs.append((word, model[word]))\n",
    "    return word_embed_pairs\n",
    "\n",
    "# input : list<(str, list<float>[300])>, str : word-vector pair list and preferred agg method.\n",
    "# output : list<float>[300] : 300-d vector that represents an aggregated value of the input words\n",
    "\n",
    "\n",
    "def SentenceEmbedUnweightedFunction(word_embed_pair_list: list, aggregateMethod: str = \"avg\"):\n",
    "    wvs = []\n",
    "    for pair in word_embed_pair_list:\n",
    "        wvs.append(pair[1])\n",
    "    if aggregateMethod == \"avg\":\n",
    "        return np.mean(wvs, axis=0)\n",
    "    else:\n",
    "        return np.sum(wvs, axis=0)\n",
    "\n",
    "# input : list<list<(str, list<float>[300])>>, str : list containing word-vector pairs and preferred agg method\n",
    "# output : list<(str, list<int>[300])> : list containing sentence-vector pairs.\n",
    "\n",
    "\n",
    "def SentenceEmbedUnweighted(word_embedded_docs: list, aggregateMethod: str = \"avg\"):\n",
    "    sentence_embedded_docs = []\n",
    "    for i in range(len(word_embedded_docs)):\n",
    "        sentence_embedded_docs.append(SentenceEmbedUnweightedFunction(\n",
    "            word_embedded_docs[i], aggregateMethod))\n",
    "    return sentence_embedded_docs\n",
    "\n",
    "\n",
    "'''\n",
    "input :\n",
    "list<list<(str, list<float>[300])>> : word-vector pair list\n",
    "matrix : tf-idf matrix for the corresponding doc\n",
    "int : the row we want\n",
    "str : preferred agg method\n",
    "'''\n",
    "# output : list<float>[300] : 300-d vector that represents an aggregated value of the input words\n",
    "\n",
    "\n",
    "def SentenceEmbedWeightedFunction(word_embed_pair_list: list, tfidf_matrix, index: int, aggregateMethod: str = \"avg\"):\n",
    "    weighted_wvs = []\n",
    "    # multiplies each word with its TF-IDF value in the corresponding row. Is 0 if word isn't found somehow.\n",
    "    for pair in word_embed_pair_list:\n",
    "        tfidf_weight = 0\n",
    "        if pair[0] in tfidf_matrix:\n",
    "            tfidf_weight = tfidf_matrix[pair[0]][index]\n",
    "        weighted_wvs.append(pair[1] * tfidf_weight)\n",
    "    # turn into array for fast aggregating\n",
    "    weighted_wvs = np.array(weighted_wvs)\n",
    "    if aggregateMethod == \"avg\":\n",
    "        sentence_vector = np.mean(weighted_wvs, axis=0)\n",
    "    else:\n",
    "        sentence_vector = np.sum(weighted_wvs, axis=0)\n",
    "    return sentence_vector\n",
    "\n",
    "# input : list<list<(str, list<float>[300])>>, str : list containing word-vector pairs, TF-IDF matrix of the corpus, and preferred agg method\n",
    "# output : list<(str, list<float>[300])> : list containing sentence-vector pairs.\n",
    "\n",
    "\n",
    "def SentenceEmbedWeighted(word_embedded_docs: list, tfidf_matrix, aggregateMethod=\"avg\"):\n",
    "    sentence_embedded_docs = []\n",
    "    for i in range(len(word_embedded_docs)):\n",
    "        sentence_embedded_docs.append(SentenceEmbedWeightedFunction(\n",
    "            word_embedded_docs[i], tfidf_matrix, i, aggregateMethod))\n",
    "    return sentence_embedded_docs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> A list of docs that will be used for everything would still be necessary, after all ðŸ‘‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>event_title</th>\n",
       "      <th>speaker</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>Document Embed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>230</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>KSMG</td>\n",
       "      <td>Pro life or pro choice?</td>\n",
       "      <td>I am pro choice. I feel bad for the baby. Peop...</td>\n",
       "      <td>[-0.022362433, 0.01713121, 0.01336585, -0.0077...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>231</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>JJ</td>\n",
       "      <td>Pro life or pro choice?</td>\n",
       "      <td>Logically, both make sense. Conflicted between...</td>\n",
       "      <td>[-0.007254202, 0.009839708, -0.00799253, -0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>232</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>JER</td>\n",
       "      <td>Pro life or pro choice?</td>\n",
       "      <td>I'm pro life. Because in my belief, if a fetus...</td>\n",
       "      <td>[-0.014276878, 0.0022278614, -0.010473907, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>233</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>YOT</td>\n",
       "      <td>Pro life or pro choice?</td>\n",
       "      <td>Prochoice. If she was a victim of rape, etc, s...</td>\n",
       "      <td>[-0.022501977, 0.024691759, 0.005002156, 0.017...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>234</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>GRE</td>\n",
       "      <td>Pro life or pro choice?</td>\n",
       "      <td>Prolife for religious reasons. Being religious...</td>\n",
       "      <td>[-0.0040696473, -0.017911343, -0.02254271, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>235</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>RIC</td>\n",
       "      <td>Pro life or pro choice?</td>\n",
       "      <td>Prochoice. Unless the baby is normal.</td>\n",
       "      <td>[-0.098942615, 0.024454754, -0.070881665, 0.05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>236</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>GRE</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>Legal - not really legal - legal for special c...</td>\n",
       "      <td>[0.004587771, -0.0013291183, 0.0049727913, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>237</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>MAR</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>Agree with other solutions besides abortion - ...</td>\n",
       "      <td>[-0.01381315, 0.01531402, 0.0022275664, -0.004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>238</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>RIC</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>Should be legal with criteria. Agree with Indo...</td>\n",
       "      <td>[-0.009043147, 0.021918792, -0.030498233, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>239</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>YOR</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>What counts as a person? Is fetus a person?</td>\n",
       "      <td>[-0.22057891, 0.014038535, -0.20972323, -0.011...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>240</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>GRE</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>Does not being a Christian still fall under Ch...</td>\n",
       "      <td>[-0.10089697, -0.104009986, -0.051932067, 0.09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>241</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>YOT</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>If it has a heartbeat, can it be called life? ...</td>\n",
       "      <td>[0.006337575, 0.01460853, -0.017543007, 0.0111...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>242</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>JJ</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>when something grows doesnt it count as life a...</td>\n",
       "      <td>[-0.052422117, 0.030819645, -0.032700323, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>243</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>STN</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>KBBI - the process of baby child teenager adul...</td>\n",
       "      <td>[0.005943503, 0.010664682, -0.022690445, -0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>244</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>YOT</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>I don't agree with Adam and Eve because not ev...</td>\n",
       "      <td>[0.016778132, 0.00512473, 0.023938367, -0.0485...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>245</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>STN</td>\n",
       "      <td>We see from our side that maybe they are miser...</td>\n",
       "      <td>Everyone has rights, even fetuses or babies, b...</td>\n",
       "      <td>[0.008111107, 0.011857094, 0.019276416, -0.016...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>246</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>GRE</td>\n",
       "      <td>We see from our side that maybe they are miser...</td>\n",
       "      <td>Is it wrong to regret life? That is the right ...</td>\n",
       "      <td>[-0.004189881, 0.0068831732, -0.020556947, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>247</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>YOT</td>\n",
       "      <td>We see from our side that maybe they are miser...</td>\n",
       "      <td>Actually, we humans are well aware that life i...</td>\n",
       "      <td>[-0.0025377192, 7.386828e-05, 0.006843238, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>248</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>STN</td>\n",
       "      <td>We see from our side that maybe they are miser...</td>\n",
       "      <td>The law didn't need to be revised. Choosing th...</td>\n",
       "      <td>[-0.0076232995, 0.0067849657, 0.038440254, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>249</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>YOT</td>\n",
       "      <td>We see from our side that maybe they are miser...</td>\n",
       "      <td>Rape victims, let's not confuse them, let's di...</td>\n",
       "      <td>[-0.010766402, 0.0011867925, -0.018306145, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index   id         event_title speaker  \\\n",
       "0       0  230  Of Choice and Life    KSMG   \n",
       "1       1  231  Of Choice and Life      JJ   \n",
       "2       2  232  Of Choice and Life     JER   \n",
       "3       3  233  Of Choice and Life     YOT   \n",
       "4       4  234  Of Choice and Life     GRE   \n",
       "5       5  235  Of Choice and Life     RIC   \n",
       "6       6  236  Of Choice and Life     GRE   \n",
       "7       7  237  Of Choice and Life     MAR   \n",
       "8       8  238  Of Choice and Life     RIC   \n",
       "9       9  239  Of Choice and Life     YOR   \n",
       "10     10  240  Of Choice and Life     GRE   \n",
       "11     11  241  Of Choice and Life     YOT   \n",
       "12     12  242  Of Choice and Life      JJ   \n",
       "13     13  243  Of Choice and Life     STN   \n",
       "14     14  244  Of Choice and Life     YOT   \n",
       "15     15  245  Of Choice and Life     STN   \n",
       "16     16  246  Of Choice and Life     GRE   \n",
       "17     17  247  Of Choice and Life     YOT   \n",
       "18     18  248  Of Choice and Life     STN   \n",
       "19     19  249  Of Choice and Life     YOT   \n",
       "\n",
       "                                             question  \\\n",
       "0                             Pro life or pro choice?   \n",
       "1                             Pro life or pro choice?   \n",
       "2                             Pro life or pro choice?   \n",
       "3                             Pro life or pro choice?   \n",
       "4                             Pro life or pro choice?   \n",
       "5                             Pro life or pro choice?   \n",
       "6              Do you think abortion should be legal?   \n",
       "7              Do you think abortion should be legal?   \n",
       "8              Do you think abortion should be legal?   \n",
       "9              Do you think abortion should be legal?   \n",
       "10             Do you think abortion should be legal?   \n",
       "11             Do you think abortion should be legal?   \n",
       "12             Do you think abortion should be legal?   \n",
       "13             Do you think abortion should be legal?   \n",
       "14             Do you think abortion should be legal?   \n",
       "15  We see from our side that maybe they are miser...   \n",
       "16  We see from our side that maybe they are miser...   \n",
       "17  We see from our side that maybe they are miser...   \n",
       "18  We see from our side that maybe they are miser...   \n",
       "19  We see from our side that maybe they are miser...   \n",
       "\n",
       "                                               answer  \\\n",
       "0   I am pro choice. I feel bad for the baby. Peop...   \n",
       "1   Logically, both make sense. Conflicted between...   \n",
       "2   I'm pro life. Because in my belief, if a fetus...   \n",
       "3   Prochoice. If she was a victim of rape, etc, s...   \n",
       "4   Prolife for religious reasons. Being religious...   \n",
       "5               Prochoice. Unless the baby is normal.   \n",
       "6   Legal - not really legal - legal for special c...   \n",
       "7   Agree with other solutions besides abortion - ...   \n",
       "8   Should be legal with criteria. Agree with Indo...   \n",
       "9         What counts as a person? Is fetus a person?   \n",
       "10  Does not being a Christian still fall under Ch...   \n",
       "11  If it has a heartbeat, can it be called life? ...   \n",
       "12  when something grows doesnt it count as life a...   \n",
       "13  KBBI - the process of baby child teenager adul...   \n",
       "14  I don't agree with Adam and Eve because not ev...   \n",
       "15  Everyone has rights, even fetuses or babies, b...   \n",
       "16  Is it wrong to regret life? That is the right ...   \n",
       "17  Actually, we humans are well aware that life i...   \n",
       "18  The law didn't need to be revised. Choosing th...   \n",
       "19  Rape victims, let's not confuse them, let's di...   \n",
       "\n",
       "                                       Document Embed  \n",
       "0   [-0.022362433, 0.01713121, 0.01336585, -0.0077...  \n",
       "1   [-0.007254202, 0.009839708, -0.00799253, -0.01...  \n",
       "2   [-0.014276878, 0.0022278614, -0.010473907, 0.0...  \n",
       "3   [-0.022501977, 0.024691759, 0.005002156, 0.017...  \n",
       "4   [-0.0040696473, -0.017911343, -0.02254271, 0.0...  \n",
       "5   [-0.098942615, 0.024454754, -0.070881665, 0.05...  \n",
       "6   [0.004587771, -0.0013291183, 0.0049727913, 0.0...  \n",
       "7   [-0.01381315, 0.01531402, 0.0022275664, -0.004...  \n",
       "8   [-0.009043147, 0.021918792, -0.030498233, -0.0...  \n",
       "9   [-0.22057891, 0.014038535, -0.20972323, -0.011...  \n",
       "10  [-0.10089697, -0.104009986, -0.051932067, 0.09...  \n",
       "11  [0.006337575, 0.01460853, -0.017543007, 0.0111...  \n",
       "12  [-0.052422117, 0.030819645, -0.032700323, -0.0...  \n",
       "13  [0.005943503, 0.010664682, -0.022690445, -0.01...  \n",
       "14  [0.016778132, 0.00512473, 0.023938367, -0.0485...  \n",
       "15  [0.008111107, 0.011857094, 0.019276416, -0.016...  \n",
       "16  [-0.004189881, 0.0068831732, -0.020556947, 0.0...  \n",
       "17  [-0.0025377192, 7.386828e-05, 0.006843238, -0....  \n",
       "18  [-0.0076232995, 0.0067849657, 0.038440254, 0.0...  \n",
       "19  [-0.010766402, 0.0011867925, -0.018306145, -0....  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [PreprocessDocument(doc, isLemma=True, isStopWords=True) for doc in df[\"answer\"]]\n",
    "word_embedded_docs = []\n",
    "for doc in docs:\n",
    "    word_embedded_docs.append(WordEmbed(doc, model))\n",
    "\n",
    "# sentence_embed(\"bababui\", tfidf_matrix=my_tfidf, index=1)\n",
    "doc_embeds = SentenceEmbedWeighted(word_embedded_docs, my_tfidf, \"avg\")\n",
    "df[\"Document Embed\"] = doc_embeds\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(inplace=True) # don't forget to add this after every row-altering operation.\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>LDA Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.015178566, 0.015176928, 0.015178298, 0.01517892, 0.015176928, 0.015178965, 0.015177403, 0.86339366, 0.015181374, 0.015178913]\n",
      "[0.8715595, 0.014267717, 0.014270598, 0.014271023, 0.014267717, 0.0142730465, 0.014269685, 0.014276249, 0.014271883, 0.014272554]\n",
      "[0.011866485, 0.011865227, 0.011866371, 0.011866245, 0.011865227, 0.011867544, 0.011865903, 0.011869738, 0.011866222, 0.8932011]\n",
      "[0.019781291, 0.01978034, 0.821959, 0.019786734, 0.01978034, 0.019783786, 0.019781467, 0.019782048, 0.0197826, 0.019782448]\n",
      "[0.014406338, 0.014404694, 0.014406862, 0.014408974, 0.014404694, 0.01441362, 0.8703269, 0.014411853, 0.014407115, 0.014409003]\n",
      "[0.030097462, 0.03009658, 0.03009934, 0.03010569, 0.03009658, 0.030103652, 0.030097155, 0.030099232, 0.72910494, 0.030099368]\n",
      "[0.015671797, 0.015671214, 0.015672458, 0.015671609, 0.015671214, 0.8589464, 0.015671996, 0.01567902, 0.015671827, 0.015672453]\n",
      "[0.018617284, 0.018616846, 0.01861919, 0.018619861, 0.018616846, 0.01862093, 0.01861727, 0.832436, 0.0186182, 0.018617587]\n",
      "[0.021005102, 0.021004882, 0.021005278, 0.021007098, 0.021004882, 0.02101101, 0.0210055, 0.8109441, 0.021006797, 0.021005388]\n",
      "[0.03092641, 0.03092524, 0.030927047, 0.030925797, 0.03092524, 0.03092602, 0.0309255, 0.7216643, 0.03092624, 0.030928174]\n",
      "[0.027838847, 0.027838647, 0.027838733, 0.027838647, 0.027838647, 0.027838837, 0.027838873, 0.7494513, 0.027838647, 0.02783889]\n",
      "[0.015906438, 0.015904922, 0.015906258, 0.015906079, 0.015904922, 0.015910769, 0.015906626, 0.015907886, 0.8568398, 0.015906291]\n",
      "[0.80662626, 0.021484952, 0.021485008, 0.02148516, 0.021484952, 0.021486562, 0.021485256, 0.02148918, 0.021486238, 0.021486444]\n",
      "[0.012575774, 0.012575189, 0.012578738, 0.012578993, 0.012575189, 0.012580878, 0.012576052, 0.17273933, 0.012580581, 0.7266393]\n",
      "[0.022880103, 0.02288003, 0.02288067, 0.022880796, 0.02288003, 0.022880593, 0.022880632, 0.7940745, 0.02288003, 0.022882586]\n",
      "[0.016989704, 0.016989578, 0.0169908, 0.01699051, 0.016989578, 0.84708554, 0.016989684, 0.016991306, 0.01699314, 0.01699014]\n",
      "[0.016180033, 0.016177343, 0.016180292, 0.01618138, 0.016177343, 0.8543803, 0.016177965, 0.016183859, 0.016179897, 0.016181605]\n",
      "[0.013030089, 0.013026951, 0.8827176, 0.013028377, 0.013026951, 0.013034541, 0.013029281, 0.013044222, 0.013030561, 0.013031416]\n",
      "[0.016423983, 0.01642277, 0.01642425, 0.8521783, 0.01642277, 0.01643073, 0.016423335, 0.016425284, 0.016424347, 0.016424261]\n",
      "[0.011440102, 0.011438417, 0.011446098, 0.89699805, 0.011438417, 0.011444645, 0.011440321, 0.011470491, 0.011440754, 0.01144269]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "input:\n",
    "- doclist : list<list<str>> --> list of tokenized sentences/docs\n",
    "- num_topics : number of inferred topics.\n",
    "'''\n",
    "'''\n",
    "output:\n",
    "- docFeatureList : list<list<float>> --> topic distribution for each sentence/doc\n",
    "'''\n",
    "def GetLDADistribution(doclist: list, topics: int = 5):\n",
    "    new_corpus = []\n",
    "    for i in range(len(docs)):\n",
    "        doc = [(j, my_matrix[i, j]) for j in my_matrix[i].indices]\n",
    "        new_corpus.append(doc)\n",
    "    gensim_dict = corpora.Dictionary.from_corpus(new_corpus)\n",
    "    lda_model = gensim.models.LdaModel(new_corpus, num_topics=10, id2word=gensim_dict)\n",
    "    goofy_ahh_doc_topic_distributions = lda_model[new_corpus]\n",
    "    docFeatureList = []\n",
    "    for doc_topic_dist in goofy_ahh_doc_topic_distributions:\n",
    "        featureList = [0.0 for i in range(0, 10)]\n",
    "        for topic_dist in doc_topic_dist:\n",
    "            featureList[topic_dist[0]] = topic_dist[1]\n",
    "        docFeatureList.append(featureList)\n",
    "    return docFeatureList\n",
    "\n",
    "# myTopicDist = GetLDADistribution(docs, 10)\n",
    "# for topicDist in myTopicDist:\n",
    "#     print(topicDist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Anomaly detection : DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "inputs:\n",
    "- vectors : list<list<float>> --> list of features corresponding to each doc/sentence\n",
    "- epsilon : float --> the radius within which points are considered connected.\n",
    "- min : int --> minimum amount of connected points for a point to be considered a core point of a cluster.\n",
    "'''\n",
    "'''\n",
    "output:\n",
    "clusters : list<int> --> a list of integers to assign each data point to a cluster. -1 means outlier.\n",
    "'''\n",
    "def GetDBSCANClusters(vectors, epsilon:float, min:int):\n",
    "    dbscan = DBSCAN(eps=epsilon, min_samples=min)\n",
    "    clusters = dbscan.fit_predict(vectors)\n",
    "    # plt.title(\"to the depths of depravity {} and the cusp of blasphemy {}.\".format(epsilon, min))\n",
    "    # plt.scatter(vectors[:, 0], vectors[:, 1], c=clusters)\n",
    "    # plt.show()\n",
    "    print(clusters)\n",
    "    return clusters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0  0  0 -1  0  0  0 -1 -1  0  0  0  0  0  0  0  0  0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0, -1,  0,  0,  0, -1, -1,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GetDBSCANClusters(list(df[\"Document Embed\"]), 1, 2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Shrink and draw with TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a tsne shrinkage also...\n",
    "def plot_documents(df:db.pd.DataFrame, isPrint=False):\n",
    "    labels = np.arange(0, df.index.stop, 1)\n",
    "    values = list(df[\"Document Embed\"]) # don't forget to list it first, then np array it later.\n",
    "\n",
    "    # train model\n",
    "    tsne_model = TSNE(perplexity=20, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(np.array(values))\n",
    "\n",
    "    # plot\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "    \n",
    "    if isPrint:\n",
    "        plt.figure(figsize=(20, 20)) \n",
    "        for i in range(len(x)):\n",
    "            plt.scatter(x[i],y[i])\n",
    "            plt.annotate(labels[i],\n",
    "                        xy=(x[i], y[i]),\n",
    "                        xytext=(5, 2),\n",
    "                        textcoords='offset points',\n",
    "                        ha='right',\n",
    "                        va='bottom')\n",
    "        plt.show()\n",
    "    # use the thing to find new clusters.\n",
    "    return new_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>event_title</th>\n",
       "      <th>speaker</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>Document Embed</th>\n",
       "      <th>Cluster Assignment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>230</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>KSMG</td>\n",
       "      <td>Pro life or pro choice?</td>\n",
       "      <td>I am pro choice. I feel bad for the baby. Peop...</td>\n",
       "      <td>[-0.022362433, 0.01713121, 0.01336585, -0.0077...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>231</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>JJ</td>\n",
       "      <td>Pro life or pro choice?</td>\n",
       "      <td>Logically, both make sense. Conflicted between...</td>\n",
       "      <td>[-0.007254202, 0.009839708, -0.00799253, -0.01...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>232</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>JER</td>\n",
       "      <td>Pro life or pro choice?</td>\n",
       "      <td>I'm pro life. Because in my belief, if a fetus...</td>\n",
       "      <td>[-0.014276878, 0.0022278614, -0.010473907, 0.0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>233</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>YOT</td>\n",
       "      <td>Pro life or pro choice?</td>\n",
       "      <td>Prochoice. If she was a victim of rape, etc, s...</td>\n",
       "      <td>[-0.022501977, 0.024691759, 0.005002156, 0.017...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>234</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>GRE</td>\n",
       "      <td>Pro life or pro choice?</td>\n",
       "      <td>Prolife for religious reasons. Being religious...</td>\n",
       "      <td>[-0.0040696473, -0.017911343, -0.02254271, 0.0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>236</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>GRE</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>Legal - not really legal - legal for special c...</td>\n",
       "      <td>[0.004587771, -0.0013291183, 0.0049727913, 0.0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>237</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>MAR</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>Agree with other solutions besides abortion - ...</td>\n",
       "      <td>[-0.01381315, 0.01531402, 0.0022275664, -0.004...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>238</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>RIC</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>Should be legal with criteria. Agree with Indo...</td>\n",
       "      <td>[-0.009043147, 0.021918792, -0.030498233, -0.0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>241</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>YOT</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>If it has a heartbeat, can it be called life? ...</td>\n",
       "      <td>[0.006337575, 0.01460853, -0.017543007, 0.0111...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>242</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>JJ</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>when something grows doesnt it count as life a...</td>\n",
       "      <td>[-0.052422117, 0.030819645, -0.032700323, -0.0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>243</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>STN</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>KBBI - the process of baby child teenager adul...</td>\n",
       "      <td>[0.005943503, 0.010664682, -0.022690445, -0.01...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>245</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>STN</td>\n",
       "      <td>We see from our side that maybe they are miser...</td>\n",
       "      <td>Everyone has rights, even fetuses or babies, b...</td>\n",
       "      <td>[0.008111107, 0.011857094, 0.019276416, -0.016...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>246</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>GRE</td>\n",
       "      <td>We see from our side that maybe they are miser...</td>\n",
       "      <td>Is it wrong to regret life? That is the right ...</td>\n",
       "      <td>[-0.004189881, 0.0068831732, -0.020556947, 0.0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>247</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>YOT</td>\n",
       "      <td>We see from our side that maybe they are miser...</td>\n",
       "      <td>Actually, we humans are well aware that life i...</td>\n",
       "      <td>[-0.0025377192, 7.386828e-05, 0.006843238, -0....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>248</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>STN</td>\n",
       "      <td>We see from our side that maybe they are miser...</td>\n",
       "      <td>The law didn't need to be revised. Choosing th...</td>\n",
       "      <td>[-0.0076232995, 0.0067849657, 0.038440254, 0.0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>249</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>YOT</td>\n",
       "      <td>We see from our side that maybe they are miser...</td>\n",
       "      <td>Rape victims, let's not confuse them, let's di...</td>\n",
       "      <td>[-0.010766402, 0.0011867925, -0.018306145, -0....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index   id         event_title speaker  \\\n",
       "0       0  230  Of Choice and Life    KSMG   \n",
       "1       1  231  Of Choice and Life      JJ   \n",
       "2       2  232  Of Choice and Life     JER   \n",
       "3       3  233  Of Choice and Life     YOT   \n",
       "4       4  234  Of Choice and Life     GRE   \n",
       "6       6  236  Of Choice and Life     GRE   \n",
       "7       7  237  Of Choice and Life     MAR   \n",
       "8       8  238  Of Choice and Life     RIC   \n",
       "11     11  241  Of Choice and Life     YOT   \n",
       "12     12  242  Of Choice and Life      JJ   \n",
       "13     13  243  Of Choice and Life     STN   \n",
       "15     15  245  Of Choice and Life     STN   \n",
       "16     16  246  Of Choice and Life     GRE   \n",
       "17     17  247  Of Choice and Life     YOT   \n",
       "18     18  248  Of Choice and Life     STN   \n",
       "19     19  249  Of Choice and Life     YOT   \n",
       "\n",
       "                                             question  \\\n",
       "0                             Pro life or pro choice?   \n",
       "1                             Pro life or pro choice?   \n",
       "2                             Pro life or pro choice?   \n",
       "3                             Pro life or pro choice?   \n",
       "4                             Pro life or pro choice?   \n",
       "6              Do you think abortion should be legal?   \n",
       "7              Do you think abortion should be legal?   \n",
       "8              Do you think abortion should be legal?   \n",
       "11             Do you think abortion should be legal?   \n",
       "12             Do you think abortion should be legal?   \n",
       "13             Do you think abortion should be legal?   \n",
       "15  We see from our side that maybe they are miser...   \n",
       "16  We see from our side that maybe they are miser...   \n",
       "17  We see from our side that maybe they are miser...   \n",
       "18  We see from our side that maybe they are miser...   \n",
       "19  We see from our side that maybe they are miser...   \n",
       "\n",
       "                                               answer  \\\n",
       "0   I am pro choice. I feel bad for the baby. Peop...   \n",
       "1   Logically, both make sense. Conflicted between...   \n",
       "2   I'm pro life. Because in my belief, if a fetus...   \n",
       "3   Prochoice. If she was a victim of rape, etc, s...   \n",
       "4   Prolife for religious reasons. Being religious...   \n",
       "6   Legal - not really legal - legal for special c...   \n",
       "7   Agree with other solutions besides abortion - ...   \n",
       "8   Should be legal with criteria. Agree with Indo...   \n",
       "11  If it has a heartbeat, can it be called life? ...   \n",
       "12  when something grows doesnt it count as life a...   \n",
       "13  KBBI - the process of baby child teenager adul...   \n",
       "15  Everyone has rights, even fetuses or babies, b...   \n",
       "16  Is it wrong to regret life? That is the right ...   \n",
       "17  Actually, we humans are well aware that life i...   \n",
       "18  The law didn't need to be revised. Choosing th...   \n",
       "19  Rape victims, let's not confuse them, let's di...   \n",
       "\n",
       "                                       Document Embed  Cluster Assignment  \n",
       "0   [-0.022362433, 0.01713121, 0.01336585, -0.0077...                   0  \n",
       "1   [-0.007254202, 0.009839708, -0.00799253, -0.01...                   0  \n",
       "2   [-0.014276878, 0.0022278614, -0.010473907, 0.0...                   0  \n",
       "3   [-0.022501977, 0.024691759, 0.005002156, 0.017...                   0  \n",
       "4   [-0.0040696473, -0.017911343, -0.02254271, 0.0...                   0  \n",
       "6   [0.004587771, -0.0013291183, 0.0049727913, 0.0...                   0  \n",
       "7   [-0.01381315, 0.01531402, 0.0022275664, -0.004...                   0  \n",
       "8   [-0.009043147, 0.021918792, -0.030498233, -0.0...                   0  \n",
       "11  [0.006337575, 0.01460853, -0.017543007, 0.0111...                   0  \n",
       "12  [-0.052422117, 0.030819645, -0.032700323, -0.0...                   0  \n",
       "13  [0.005943503, 0.010664682, -0.022690445, -0.01...                   0  \n",
       "15  [0.008111107, 0.011857094, 0.019276416, -0.016...                   0  \n",
       "16  [-0.004189881, 0.0068831732, -0.020556947, 0.0...                   0  \n",
       "17  [-0.0025377192, 7.386828e-05, 0.006843238, -0....                   0  \n",
       "18  [-0.0076232995, 0.0067849657, 0.038440254, 0.0...                   0  \n",
       "19  [-0.010766402, 0.0011867925, -0.018306145, -0....                   0  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clusters = GetDBSCANClusters(list(df[\"Document Embed\"]), 0.6, 5)\n",
    "# df[\"Cluster Assignment\"] = clusters\n",
    "# dfSupposedOutliers = df.loc[df[\"Cluster Assignment\"] == -1]\n",
    "# dfSupposedGoods = df.loc[df[\"Cluster Assignment\"] != -1]\n",
    "# dfSupposedGoods.reset_index(inplace=True)\n",
    "# dfSupposedOutliers.reset_index(inplace=True)\n",
    "\n",
    "# for i in range(len(dfSupposedOutliers.index)):\n",
    "#     print(dfSupposedOutliers.loc[i][\"question\"], \" | \", dfSupposedOutliers.loc[i][\"answer\"])\n",
    "\n",
    "# print(\"#\" * 80)\n",
    "# for i in range(len(dfSupposedGoods.index)):\n",
    "#     print(dfSupposedGoods.loc[i][\"question\"], \" | \", dfSupposedGoods.loc[i][\"answer\"])\n",
    "\n",
    "'''\n",
    "inputs :\n",
    "- clusters : list<int> --> a list of clusters assigned to each doc/sentence\n",
    "- df : DataFrame --> the dataframe in question\n",
    "'''\n",
    "'''\n",
    "outputs:\n",
    "- dfOutliers : DataFrame --> the dataframe whose answers have been marked as outliers.\n",
    "- dfGoods : DataFrame --> the dataframe whose answers have not been marked as outliers.\n",
    "'''\n",
    "def ReturnClusters(clusters:list, df:db.pd.DataFrame):\n",
    "    df[\"Cluster Assignment\"] = clusters\n",
    "    dfGoods = df.loc[df[\"Cluster Assignment\"] != -1]\n",
    "    dfOutliers = df.loc[df[\"Cluster Assignment\"] == -1]\n",
    "    return dfOutliers, dfGoods\n",
    "\n",
    "# outliers, goods = ReturnClusters(clusters, df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Anomaly detection : LOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Anomaly detection : Isolation Forest (sklearn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Final Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyDetector():\n",
    "    def __init__(self, eventID: int, isSplit: bool) -> None:\n",
    "        self.dh = db.DatabaseHandler(\"test.db\")\n",
    "        self.df = self.GetDF(self.dh, \"eventID\", eventID, isSplit)\n",
    "        self.model = api.load(\"glove-wiki-gigaword-300\")\n",
    "        pass\n",
    "\n",
    "    # input : DatabaseHandler\n",
    "    # output : DataFrame\n",
    "    def GetDF(self, dh: db.DatabaseHandler, selector: str, eventID: int, splitBySentences: bool = False):\n",
    "        df = dh.get_recordDataJoinedDF(selector=selector, ID=eventID)\n",
    "        if splitBySentences:\n",
    "            # df.set_index('id', inplace=True)\n",
    "            df['answer'] = df['answer'].str.split('.')\n",
    "            df = df.explode(\"answer\", True)\n",
    "            df.drop(df[df[\"answer\"] == \"\"].index, inplace=True)\n",
    "            df.reset_index(drop=True, inplace=True)\n",
    "        return df\n",
    "\n",
    "    # input : sentence/document (string); parameters\n",
    "    # output : a list of word tokens (list<string>)\n",
    "    def PreprocessDocument(self, doc: str, isLemma: bool = False, isStopWords: bool = False, isInflect: bool = False, isNumberFiltered: bool = True):\n",
    "        inflector = inflect.engine()\n",
    "        stopwordSet = set(stopwords.words(\"english\"))\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        punctuations = string.punctuation\n",
    "        # if numbers are filtered, add that to the punctuation string\n",
    "        if isNumberFiltered:\n",
    "            punctuations += \"1234567890\"\n",
    "\n",
    "        # case fold\n",
    "        doc = doc.lower()\n",
    "\n",
    "        # remove puncs\n",
    "        doc = \"\".join([char for char in doc if char not in punctuations])\n",
    "\n",
    "        # tokenize it.\n",
    "        token_list = nltk.word_tokenize(doc)\n",
    "\n",
    "        for i in range(len(token_list)):\n",
    "            # if inflect\n",
    "            if isInflect:\n",
    "                if token_list[i].isdigit():\n",
    "                    token_list[i] = inflector.number_to_words(token_list[i])\n",
    "\n",
    "            # if lemma\n",
    "            if isLemma:\n",
    "                tagged_word = nltk.pos_tag([token_list[i]])\n",
    "                wordnet_pos = getWordnetPos(tagged_word[0][1])\n",
    "                token_list[i] = lemmatizer.lemmatize(\n",
    "                    tagged_word[0][0], pos=wordnet_pos)\n",
    "\n",
    "            # if stopword\n",
    "            if isStopWords:\n",
    "                if token_list[i] in stopwordSet or token_list[i].isdigit():\n",
    "                    token_list[i] = \"#\"  # mark as #\n",
    "\n",
    "        # remove the marked strings\n",
    "        token_list = [token for token in token_list if token != \"#\"]\n",
    "\n",
    "        if token_list:\n",
    "            return token_list\n",
    "        return [\"\"]\n",
    "\n",
    "    def getWordnetPos(tag):\n",
    "        \"\"\"Map POS tag to WordNet POS tag\"\"\"\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN  # solves as noun by default.\n",
    "\n",
    "    # input : list<str>\n",
    "    # output : Dataframe, matrix\n",
    "    def GetTFIDF(self, doclist: list, isPreprocessed=True):\n",
    "        if not isPreprocessed:\n",
    "            doclist = [PreprocessDocument(\n",
    "                doc, isLemma=True, isStopWords=True) for doc in doclist]\n",
    "        else:\n",
    "            # just tokenize the thing\n",
    "            doclist = [nltk.word_tokenize(doc) for doc in doclist]\n",
    "\n",
    "        flat_doclist = [' '.join(doc)\n",
    "                        for doc in doclist]  # turn into one big corpus\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        matrix = vectorizer.fit_transform(flat_doclist)\n",
    "        tfidf_keys = vectorizer.get_feature_names_out()\n",
    "        df_tfidf = db.pd.DataFrame(matrix.toarray(), columns=tfidf_keys)\n",
    "\n",
    "        return df_tfidf, matrix\n",
    "\n",
    "    # input : list<str> : tokens of one document/sentence\n",
    "    # output : list<(str, list<int>[300])> : list of word-vector pair for each word available on the model\n",
    "    def WordEmbed(self, document: list, model):\n",
    "        word_embed_pairs = []\n",
    "        for word in document:\n",
    "            if word in model:\n",
    "                word_embed_pairs.append((word, model[word]))\n",
    "        return word_embed_pairs\n",
    "\n",
    "    # input : list<(str, list<float>[300])>, str : word-vector pair list and preferred agg method.\n",
    "    # output : list<float>[300] : 300-d vector that represents an aggregated value of the input words\n",
    "    def SentenceEmbedUnweightedFunction(self, word_embed_pair_list: list, aggregateMethod: str = \"avg\"):\n",
    "        wvs = []\n",
    "        for pair in word_embed_pair_list:\n",
    "            wvs.append(pair[1])\n",
    "        if aggregateMethod == \"avg\":\n",
    "            return np.mean(wvs, axis=0)\n",
    "        else:\n",
    "            return np.sum(wvs, axis=0)\n",
    "\n",
    "    # input : list<list<(str, list<float>[300])>>, str : list containing word-vector pairs and preferred agg method\n",
    "    # output : list<(str, list<int>[300])> : list containing sentence-vector pairs.\n",
    "    def SentenceEmbedUnweighted(self, word_embedded_docs: list, aggregateMethod: str = \"avg\"):\n",
    "        sentence_embedded_docs = []\n",
    "        for i in range(len(word_embedded_docs)):\n",
    "            sentence_embedded_docs.append(SentenceEmbedUnweightedFunction(\n",
    "                word_embedded_docs[i], aggregateMethod))\n",
    "        return sentence_embedded_docs\n",
    "\n",
    "    '''\n",
    "    input :\n",
    "    list<list<(str, list<float>[300])>> : word-vector pair list\n",
    "    matrix : tf-idf matrix for the corresponding doc\n",
    "    int : the row we want\n",
    "    str : preferred agg method\n",
    "    '''\n",
    "    # output : list<float>[300] : 300-d vector that represents an aggregated value of the input words\n",
    "    def SentenceEmbedWeightedFunction(self, word_embed_pair_list: list, tfidf_matrix, index: int, aggregateMethod: str = \"avg\"):\n",
    "        weighted_wvs = []\n",
    "        # multiplies each word with its TF-IDF value in the corresponding row. Is 0 if word isn't found somehow.\n",
    "        for pair in word_embed_pair_list:\n",
    "            tfidf_weight = 0\n",
    "            if pair[0] in tfidf_matrix:\n",
    "                tfidf_weight = tfidf_matrix[pair[0]][index]\n",
    "            weighted_wvs.append(pair[1] * tfidf_weight)\n",
    "        # turn into array for fast aggregating\n",
    "        weighted_wvs = np.array(weighted_wvs)\n",
    "        if aggregateMethod == \"avg\":\n",
    "            sentence_vector = np.mean(weighted_wvs, axis=0)\n",
    "        else:\n",
    "            sentence_vector = np.sum(weighted_wvs, axis=0)\n",
    "        return sentence_vector\n",
    "\n",
    "    # input : list<list<(str, list<float>[300])>>, str : list containing word-vector pairs, TF-IDF matrix of the corpus, and preferred agg method\n",
    "    # output : list<(str, list<float>[300])> : list containing sentence-vector pairs.\n",
    "    def SentenceEmbedWeighted(self, word_embedded_docs: list, tfidf_matrix, aggregateMethod=\"avg\"):\n",
    "        sentence_embedded_docs = []\n",
    "        for i in range(len(word_embedded_docs)):\n",
    "            sentence_embedded_docs.append(SentenceEmbedWeightedFunction(\n",
    "                word_embedded_docs[i], tfidf_matrix, i, aggregateMethod))\n",
    "        return sentence_embedded_docs\n",
    "\n",
    "    '''\n",
    "    input:\n",
    "    - doclist : list<list<str>> --> list of tokenized sentences/docs\n",
    "    - num_topics : number of inferred topics.\n",
    "    '''\n",
    "    '''\n",
    "    output:\n",
    "    - docFeatureList : list<list<float>> --> topic distribution for each sentence/doc\n",
    "    '''\n",
    "    def GetLDADistribution(self, doclist: list, topics: int = 5):\n",
    "        new_corpus = []\n",
    "        for i in range(len(docs)):\n",
    "            doc = [(j, my_matrix[i, j]) for j in my_matrix[i].indices]\n",
    "            new_corpus.append(doc)\n",
    "        gensim_dict = corpora.Dictionary.from_corpus(new_corpus)\n",
    "        lda_model = gensim.models.LdaModel(\n",
    "            new_corpus, num_topics=10, id2word=gensim_dict)\n",
    "        goofy_ahh_doc_topic_distributions = lda_model[new_corpus]\n",
    "        docFeatureList = []\n",
    "        for doc_topic_dist in goofy_ahh_doc_topic_distributions:\n",
    "            featureList = [0.0 for i in range(0, 10)]\n",
    "            for topic_dist in doc_topic_dist:\n",
    "                featureList[topic_dist[0]] = topic_dist[1]\n",
    "            docFeatureList.append(featureList)\n",
    "        return docFeatureList\n",
    "    \n",
    "    '''\n",
    "    inputs:\n",
    "    - vectors : list<list<float>> --> list of features corresponding to each doc/sentence\n",
    "    - epsilon : float --> the radius within which points are considered connected.\n",
    "    - min : int --> minimum amount of connected points for a point to be considered a core point of a cluster.\n",
    "    '''\n",
    "    '''\n",
    "    output:\n",
    "    clusters : list<int> --> a list of integers to assign each data point to a cluster. -1 means outlier.\n",
    "    '''\n",
    "    def GetDBSCANClusters(self, vectors, epsilon:float, min:int):\n",
    "        dbscan = DBSCAN(eps=epsilon, min_samples=min)\n",
    "        clusters = dbscan.fit_predict(vectors)\n",
    "        # plt.title(\"to the depths of depravity {} and the cusp of blasphemy {}.\".format(epsilon, min))\n",
    "        # plt.scatter(vectors[:, 0], vectors[:, 1], c=clusters)\n",
    "        # plt.show()\n",
    "        print(clusters)\n",
    "        return clusters\n",
    "    \n",
    "    '''\n",
    "    inputs :\n",
    "    - clusters : list<int> --> a list of clusters assigned to each doc/sentence\n",
    "    - df : DataFrame --> the dataframe in question\n",
    "    '''\n",
    "    '''\n",
    "    outputs:\n",
    "    - dfOutliers : DataFrame --> the dataframe whose answers have been marked as outliers.\n",
    "    - dfGoods : DataFrame --> the dataframe whose answers have not been marked as outliers.\n",
    "    '''\n",
    "    def ReturnClusters(self, clusters:list, df:db.pd.DataFrame):\n",
    "        df[\"Cluster Assignment\"] = clusters\n",
    "        dfGoods = df.loc[df[\"Cluster Assignment\"] != -1]\n",
    "        dfOutliers = df.loc[df[\"Cluster Assignment\"] == -1]\n",
    "        return dfOutliers, dfGoods\n",
    "\n",
    "    def GetAnomalies(self, method: str, model, isWeighted: bool = True, aggregateMethod: str = \"avg\", epsilon=0.01, minsamp=2, topics=5):\n",
    "        # initialize\n",
    "        pass\n",
    "        # extract the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "efebef93848af86820ba5c9af15e3b0ea109bf901e8b1e27dbaca7b722da8278"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
