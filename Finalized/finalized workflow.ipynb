{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>IMPORTS</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports go here\n",
    "import numpy as np\n",
    "import db\n",
    "import inflect\n",
    "import string\n",
    "import nltk\n",
    "import gensim\n",
    "import contractions\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim.downloader as api\n",
    "from gensim import corpora, models\n",
    "from nltk.test.gensim_fixt import setup_module\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Init (don't forget to do this when making the class, and only once.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_module()\n",
    "model = api.load(\"glove-wiki-gigaword-300\")\n",
    "\n",
    "# see here : https://radimrehurek.com/gensim/downloader.html for saving."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Retrieval</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>event_title</th>\n",
       "      <th>speaker</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>230</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>KSMG</td>\n",
       "      <td>Pro life or pro choice?</td>\n",
       "      <td>I am pro choice. I feel bad for the baby. Peop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>231</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>JJ</td>\n",
       "      <td>Pro life or pro choice?</td>\n",
       "      <td>Logically, both make sense. Conflicted between...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>232</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>JER</td>\n",
       "      <td>Pro life or pro choice?</td>\n",
       "      <td>I'm pro life. Because in my belief, if a fetus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>233</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>YOT</td>\n",
       "      <td>Pro life or pro choice?</td>\n",
       "      <td>Prochoice. If she was a victim of rape, etc, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>234</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>GRE</td>\n",
       "      <td>Pro life or pro choice?</td>\n",
       "      <td>Prolife for religious reasons. Being religious...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>235</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>RIC</td>\n",
       "      <td>Pro life or pro choice?</td>\n",
       "      <td>Prochoice. Unless the baby is normal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>236</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>GRE</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>Legal - not really legal - legal for special c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>237</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>MAR</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>Agree with other solutions besides abortion - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>238</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>RIC</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>Should be legal with criteria. Agree with Indo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>239</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>YOR</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>What counts as a person? Is fetus a person?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>240</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>GRE</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>Does not being a Christian still fall under Ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>241</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>YOT</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>If it has a heartbeat, can it be called life? ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>242</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>JJ</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>when something grows doesnt it count as life a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>243</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>STN</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>KBBI - the process of baby child teenager adul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>244</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>YOT</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>I don't agree with Adam and Eve because not ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>245</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>STN</td>\n",
       "      <td>We see from our side that maybe they are miser...</td>\n",
       "      <td>Everyone has rights, even fetuses or babies, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>246</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>GRE</td>\n",
       "      <td>We see from our side that maybe they are miser...</td>\n",
       "      <td>Is it wrong to regret life? That is the right ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>247</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>YOT</td>\n",
       "      <td>We see from our side that maybe they are miser...</td>\n",
       "      <td>Actually, we humans are well aware that life i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>248</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>STN</td>\n",
       "      <td>We see from our side that maybe they are miser...</td>\n",
       "      <td>The law didn't need to be revised. Choosing th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>249</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>YOT</td>\n",
       "      <td>We see from our side that maybe they are miser...</td>\n",
       "      <td>Rape victims, let's not confuse them, let's di...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id         event_title speaker  \\\n",
       "0   230  Of Choice and Life    KSMG   \n",
       "1   231  Of Choice and Life      JJ   \n",
       "2   232  Of Choice and Life     JER   \n",
       "3   233  Of Choice and Life     YOT   \n",
       "4   234  Of Choice and Life     GRE   \n",
       "5   235  Of Choice and Life     RIC   \n",
       "6   236  Of Choice and Life     GRE   \n",
       "7   237  Of Choice and Life     MAR   \n",
       "8   238  Of Choice and Life     RIC   \n",
       "9   239  Of Choice and Life     YOR   \n",
       "10  240  Of Choice and Life     GRE   \n",
       "11  241  Of Choice and Life     YOT   \n",
       "12  242  Of Choice and Life      JJ   \n",
       "13  243  Of Choice and Life     STN   \n",
       "14  244  Of Choice and Life     YOT   \n",
       "15  245  Of Choice and Life     STN   \n",
       "16  246  Of Choice and Life     GRE   \n",
       "17  247  Of Choice and Life     YOT   \n",
       "18  248  Of Choice and Life     STN   \n",
       "19  249  Of Choice and Life     YOT   \n",
       "\n",
       "                                             question  \\\n",
       "0                             Pro life or pro choice?   \n",
       "1                             Pro life or pro choice?   \n",
       "2                             Pro life or pro choice?   \n",
       "3                             Pro life or pro choice?   \n",
       "4                             Pro life or pro choice?   \n",
       "5                             Pro life or pro choice?   \n",
       "6              Do you think abortion should be legal?   \n",
       "7              Do you think abortion should be legal?   \n",
       "8              Do you think abortion should be legal?   \n",
       "9              Do you think abortion should be legal?   \n",
       "10             Do you think abortion should be legal?   \n",
       "11             Do you think abortion should be legal?   \n",
       "12             Do you think abortion should be legal?   \n",
       "13             Do you think abortion should be legal?   \n",
       "14             Do you think abortion should be legal?   \n",
       "15  We see from our side that maybe they are miser...   \n",
       "16  We see from our side that maybe they are miser...   \n",
       "17  We see from our side that maybe they are miser...   \n",
       "18  We see from our side that maybe they are miser...   \n",
       "19  We see from our side that maybe they are miser...   \n",
       "\n",
       "                                               answer  \n",
       "0   I am pro choice. I feel bad for the baby. Peop...  \n",
       "1   Logically, both make sense. Conflicted between...  \n",
       "2   I'm pro life. Because in my belief, if a fetus...  \n",
       "3   Prochoice. If she was a victim of rape, etc, s...  \n",
       "4   Prolife for religious reasons. Being religious...  \n",
       "5               Prochoice. Unless the baby is normal.  \n",
       "6   Legal - not really legal - legal for special c...  \n",
       "7   Agree with other solutions besides abortion - ...  \n",
       "8   Should be legal with criteria. Agree with Indo...  \n",
       "9         What counts as a person? Is fetus a person?  \n",
       "10  Does not being a Christian still fall under Ch...  \n",
       "11  If it has a heartbeat, can it be called life? ...  \n",
       "12  when something grows doesnt it count as life a...  \n",
       "13  KBBI - the process of baby child teenager adul...  \n",
       "14  I don't agree with Adam and Eve because not ev...  \n",
       "15  Everyone has rights, even fetuses or babies, b...  \n",
       "16  Is it wrong to regret life? That is the right ...  \n",
       "17  Actually, we humans are well aware that life i...  \n",
       "18  The law didn't need to be revised. Choosing th...  \n",
       "19  Rape victims, let's not confuse them, let's di...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input : DatabaseHandler\n",
    "# output : DataFrame\n",
    "def GetDF(dh:db.DatabaseHandler, selector: str, eventID: int, splitBySentences: bool = False):\n",
    "    df = dh.get_recordDataJoinedDF(selector=selector, ID=eventID)\n",
    "    if splitBySentences:\n",
    "        # df.set_index('id', inplace=True)\n",
    "        df['answer'] = df['answer'].str.split('.')\n",
    "        df = df.explode(\"answer\", True)\n",
    "        df.drop(df[df[\"answer\"] == \"\"].index, inplace=True)\n",
    "        df.reset_index(drop=True,inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "dh = db.DatabaseHandler(\"testdb.db\")  # db connection\n",
    "df = GetDF(dh, \"event_id\", 20, False)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Preprocessing</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input : sentence/document (string); parameters\n",
    "# output : a list of word tokens (list<string>)\n",
    "def PreprocessDocument(doc:str, isLemma:bool=False, isStopWords:bool=False, isInflect:bool=False, isNumberFiltered:bool=True):\n",
    "    inflector = inflect.engine()\n",
    "    stopwordSet = set(stopwords.words(\"english\"))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    punctuations = string.punctuation\n",
    "    # if numbers are filtered, add that to the punctuation string\n",
    "    if isNumberFiltered:\n",
    "        punctuations += \"1234567890\"\n",
    "\n",
    "    # case fold\n",
    "    doc = doc.lower()\n",
    "\n",
    "    # remove puncs\n",
    "    doc = \"\".join([char for char in doc if char not in punctuations])\n",
    "\n",
    "    # tokenize it.\n",
    "    token_list = nltk.word_tokenize(doc)\n",
    "\n",
    "    for i in range(len(token_list)):\n",
    "        # if inflect\n",
    "        if isInflect:\n",
    "            if token_list[i].isdigit():\n",
    "                token_list[i] = inflector.number_to_words(token_list[i])\n",
    "\n",
    "        # if lemma\n",
    "        if isLemma:\n",
    "            tagged_word = nltk.pos_tag([token_list[i]])\n",
    "            wordnet_pos = getWordnetPos(tagged_word[0][1])\n",
    "            token_list[i] = lemmatizer.lemmatize(tagged_word[0][0], pos=wordnet_pos)\n",
    "        \n",
    "        # if stopword\n",
    "        if isStopWords:\n",
    "            if token_list[i] in stopwordSet or token_list[i].isdigit():\n",
    "                token_list[i] = \"#\" # mark as #\n",
    "        \n",
    "    # remove the marked strings\n",
    "    token_list = [token for token in token_list if token != \"#\"]\n",
    "\n",
    "    if token_list:\n",
    "        return token_list\n",
    "    return [\"\"]\n",
    "\n",
    "def getWordnetPos(tag):\n",
    "    \"\"\"Map POS tag to WordNet POS tag\"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN # solves as noun by default.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['christian', 'still', 'fall', 'christian', 'rule']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mySentence = df.loc[10][\"answer\"]\n",
    "myTokenizedSentence = PreprocessDocument(mySentence, isStopWords=True, isLemma=True)\n",
    "myTokenizedSentence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Get word set, whatever for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288\n"
     ]
    }
   ],
   "source": [
    "def get_word_set(df):\n",
    "    bigtext = \"\"\n",
    "    # join in lower case\n",
    "    for i in range(len(df)):\n",
    "        bigtext += \" {}\".format(df[i].lower())\n",
    "    bigtext = contractions.fix(bigtext) # remove contractions\n",
    "    bigtext = \"\".join([char for char in bigtext if char not in string.punctuation]) # remove punctuations\n",
    "    big_text_tokens = PreprocessDocument(bigtext, isLemma=True) # put in blender like dick\n",
    "    return set(big_text_tokens) # return as set\n",
    "\n",
    "myWordSet = get_word_set(df[\"answer\"])\n",
    "print(len(myWordSet))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> TF-IDF function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input : list<str>\n",
    "# output : Dataframe, matrix\n",
    "def GetTFIDF(doclist:list, isPreprocessed=True):\n",
    "    if not isPreprocessed:\n",
    "        doclist = [PreprocessDocument(doc, isLemma=True, isStopWords=True) for doc in doclist]\n",
    "    else:\n",
    "        # just tokenize the thing\n",
    "        doclist = [nltk.word_tokenize(doc) for doc in doclist]\n",
    "        \n",
    "    flat_doclist = [' '.join(doc) for doc in doclist] # turn into one big corpus\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    matrix  = vectorizer.fit_transform(flat_doclist)\n",
    "    tfidf_keys = vectorizer.get_feature_names_out()\n",
    "    df_tfidf = db.pd.DataFrame(matrix.toarray(), columns=tfidf_keys)\n",
    "\n",
    "    return df_tfidf, matrix\n",
    "\n",
    "\n",
    "def GetTFIDF_Gensim(doclist:list, isPreprocessed=True):\n",
    "    if not isPreprocessed:\n",
    "        doclist = [PreprocessDocument(doc, isLemma=True, isStopWords=True, isInflect=True) for doc in doclist]\n",
    "    else:\n",
    "        doclist = [nltk.word_tokenize(doc) for doc in doclist]\n",
    "    dictionary = corpora.Dictionary(doclist)\n",
    "\n",
    "    return dictionary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tfidf, my_matrix = GetTFIDF(df[\"answer\"], isPreprocessed=True)\n",
    "my_tfidf\n",
    "\n",
    "goofy = GetTFIDF_Gensim(df[\"answer\"], False)\n",
    "len(goofy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Word- and Sentence-Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input : list<str> : tokens of one document/sentence\n",
    "# output : list<(str, list<int>[300])> : list of word-vector pair for each word available on the model\n",
    "def WordEmbed(document: list, model):\n",
    "    word_embed_pairs = []\n",
    "    for word in document:\n",
    "        if word in model:\n",
    "            word_embed_pairs.append((word, model[word]))\n",
    "    return word_embed_pairs\n",
    "\n",
    "# input : list<(str, list<float>[300])>, str : word-vector pair list and preferred agg method.\n",
    "# output : list<float>[300] : 300-d vector that represents an aggregated value of the input words\n",
    "\n",
    "\n",
    "def SentenceEmbedUnweightedFunction(word_embed_pair_list: list, aggregateMethod: str = \"avg\"):\n",
    "    wvs = []\n",
    "    for pair in word_embed_pair_list:\n",
    "        wvs.append(pair[1])\n",
    "    if aggregateMethod == \"avg\":\n",
    "        return np.mean(wvs, axis=0)\n",
    "    else:\n",
    "        return np.sum(wvs, axis=0)\n",
    "\n",
    "# input : list<list<(str, list<float>[300])>>, str : list containing word-vector pairs and preferred agg method\n",
    "# output : list<(str, list<int>[300])> : list containing sentence-vector pairs.\n",
    "\n",
    "\n",
    "def SentenceEmbedUnweighted(word_embedded_docs: list, aggregateMethod: str = \"avg\"):\n",
    "    sentence_embedded_docs = []\n",
    "    for i in range(len(word_embedded_docs)):\n",
    "        sentence_embedded_docs.append(SentenceEmbedUnweightedFunction(\n",
    "            word_embedded_docs[i], aggregateMethod))\n",
    "    return sentence_embedded_docs\n",
    "\n",
    "\n",
    "'''\n",
    "input :\n",
    "list<list<(str, list<float>[300])>> : word-vector pair list\n",
    "matrix : tf-idf matrix for the corresponding doc\n",
    "int : the row we want\n",
    "str : preferred agg method\n",
    "'''\n",
    "# output : list<float>[300] : 300-d vector that represents an aggregated value of the input words\n",
    "\n",
    "\n",
    "def SentenceEmbedWeightedFunction(word_embed_pair_list: list, tfidf_matrix, index: int, aggregateMethod: str = \"avg\"):\n",
    "    weighted_wvs = []\n",
    "    # multiplies each word with its TF-IDF value in the corresponding row. Is 0 if word isn't found somehow.\n",
    "    for pair in word_embed_pair_list:\n",
    "        tfidf_weight = 0\n",
    "        if pair[0] in tfidf_matrix:\n",
    "            tfidf_weight = tfidf_matrix[pair[0]][index]\n",
    "        weighted_wvs.append(pair[1] * tfidf_weight)\n",
    "    # turn into array for fast aggregating\n",
    "    weighted_wvs = np.array(weighted_wvs)\n",
    "    if aggregateMethod == \"avg\":\n",
    "        sentence_vector = np.mean(weighted_wvs, axis=0)\n",
    "    else:\n",
    "        sentence_vector = np.sum(weighted_wvs, axis=0)\n",
    "    return sentence_vector\n",
    "\n",
    "# input : list<list<(str, list<float>[300])>>, str : list containing word-vector pairs, TF-IDF matrix of the corpus, and preferred agg method\n",
    "# output : list<(str, list<float>[300])> : list containing sentence-vector pairs.\n",
    "\n",
    "\n",
    "def SentenceEmbedWeighted(word_embedded_docs: list, tfidf_matrix, aggregateMethod=\"avg\"):\n",
    "    sentence_embedded_docs = []\n",
    "    for i in range(len(word_embedded_docs)):\n",
    "        sentence_embedded_docs.append(SentenceEmbedWeightedFunction(\n",
    "            word_embedded_docs[i], tfidf_matrix, i, aggregateMethod))\n",
    "    return sentence_embedded_docs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> A list of docs that will be used for everything would still be necessary, after all 👇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>event_title</th>\n",
       "      <th>speaker</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>Document Embed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>230</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>KSMG</td>\n",
       "      <td>Pro life or pro choice?</td>\n",
       "      <td>I am pro choice. I feel bad for the baby. Peop...</td>\n",
       "      <td>[-0.022362433, 0.01713121, 0.01336585, -0.0077...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>231</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>JJ</td>\n",
       "      <td>Pro life or pro choice?</td>\n",
       "      <td>Logically, both make sense. Conflicted between...</td>\n",
       "      <td>[-0.007254202, 0.009839708, -0.00799253, -0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>232</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>JER</td>\n",
       "      <td>Pro life or pro choice?</td>\n",
       "      <td>I'm pro life. Because in my belief, if a fetus...</td>\n",
       "      <td>[-0.014276878, 0.0022278614, -0.010473907, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>233</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>YOT</td>\n",
       "      <td>Pro life or pro choice?</td>\n",
       "      <td>Prochoice. If she was a victim of rape, etc, s...</td>\n",
       "      <td>[-0.022501977, 0.024691759, 0.005002156, 0.017...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>234</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>GRE</td>\n",
       "      <td>Pro life or pro choice?</td>\n",
       "      <td>Prolife for religious reasons. Being religious...</td>\n",
       "      <td>[-0.0040696473, -0.017911343, -0.02254271, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>235</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>RIC</td>\n",
       "      <td>Pro life or pro choice?</td>\n",
       "      <td>Prochoice. Unless the baby is normal.</td>\n",
       "      <td>[-0.098942615, 0.024454754, -0.070881665, 0.05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>236</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>GRE</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>Legal - not really legal - legal for special c...</td>\n",
       "      <td>[0.004587771, -0.0013291183, 0.0049727913, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>237</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>MAR</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>Agree with other solutions besides abortion - ...</td>\n",
       "      <td>[-0.01381315, 0.01531402, 0.0022275664, -0.004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>238</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>RIC</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>Should be legal with criteria. Agree with Indo...</td>\n",
       "      <td>[-0.009043147, 0.021918792, -0.030498233, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>239</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>YOR</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>What counts as a person? Is fetus a person?</td>\n",
       "      <td>[-0.22057891, 0.014038535, -0.20972323, -0.011...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>240</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>GRE</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>Does not being a Christian still fall under Ch...</td>\n",
       "      <td>[-0.10089697, -0.104009986, -0.051932067, 0.09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>241</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>YOT</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>If it has a heartbeat, can it be called life? ...</td>\n",
       "      <td>[0.006337575, 0.01460853, -0.017543007, 0.0111...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>242</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>JJ</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>when something grows doesnt it count as life a...</td>\n",
       "      <td>[-0.052422117, 0.030819645, -0.032700323, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>243</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>STN</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>KBBI - the process of baby child teenager adul...</td>\n",
       "      <td>[0.005943503, 0.010664682, -0.022690445, -0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>244</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>YOT</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>I don't agree with Adam and Eve because not ev...</td>\n",
       "      <td>[0.016778132, 0.00512473, 0.023938367, -0.0485...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>245</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>STN</td>\n",
       "      <td>We see from our side that maybe they are miser...</td>\n",
       "      <td>Everyone has rights, even fetuses or babies, b...</td>\n",
       "      <td>[0.008111107, 0.011857094, 0.019276416, -0.016...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>246</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>GRE</td>\n",
       "      <td>We see from our side that maybe they are miser...</td>\n",
       "      <td>Is it wrong to regret life? That is the right ...</td>\n",
       "      <td>[-0.004189881, 0.0068831732, -0.020556947, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>247</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>YOT</td>\n",
       "      <td>We see from our side that maybe they are miser...</td>\n",
       "      <td>Actually, we humans are well aware that life i...</td>\n",
       "      <td>[-0.0025377192, 7.386828e-05, 0.006843238, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>248</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>STN</td>\n",
       "      <td>We see from our side that maybe they are miser...</td>\n",
       "      <td>The law didn't need to be revised. Choosing th...</td>\n",
       "      <td>[-0.0076232995, 0.0067849657, 0.038440254, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>249</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>YOT</td>\n",
       "      <td>We see from our side that maybe they are miser...</td>\n",
       "      <td>Rape victims, let's not confuse them, let's di...</td>\n",
       "      <td>[-0.010766402, 0.0011867925, -0.018306145, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index   id         event_title speaker  \\\n",
       "0       0  230  Of Choice and Life    KSMG   \n",
       "1       1  231  Of Choice and Life      JJ   \n",
       "2       2  232  Of Choice and Life     JER   \n",
       "3       3  233  Of Choice and Life     YOT   \n",
       "4       4  234  Of Choice and Life     GRE   \n",
       "5       5  235  Of Choice and Life     RIC   \n",
       "6       6  236  Of Choice and Life     GRE   \n",
       "7       7  237  Of Choice and Life     MAR   \n",
       "8       8  238  Of Choice and Life     RIC   \n",
       "9       9  239  Of Choice and Life     YOR   \n",
       "10     10  240  Of Choice and Life     GRE   \n",
       "11     11  241  Of Choice and Life     YOT   \n",
       "12     12  242  Of Choice and Life      JJ   \n",
       "13     13  243  Of Choice and Life     STN   \n",
       "14     14  244  Of Choice and Life     YOT   \n",
       "15     15  245  Of Choice and Life     STN   \n",
       "16     16  246  Of Choice and Life     GRE   \n",
       "17     17  247  Of Choice and Life     YOT   \n",
       "18     18  248  Of Choice and Life     STN   \n",
       "19     19  249  Of Choice and Life     YOT   \n",
       "\n",
       "                                             question  \\\n",
       "0                             Pro life or pro choice?   \n",
       "1                             Pro life or pro choice?   \n",
       "2                             Pro life or pro choice?   \n",
       "3                             Pro life or pro choice?   \n",
       "4                             Pro life or pro choice?   \n",
       "5                             Pro life or pro choice?   \n",
       "6              Do you think abortion should be legal?   \n",
       "7              Do you think abortion should be legal?   \n",
       "8              Do you think abortion should be legal?   \n",
       "9              Do you think abortion should be legal?   \n",
       "10             Do you think abortion should be legal?   \n",
       "11             Do you think abortion should be legal?   \n",
       "12             Do you think abortion should be legal?   \n",
       "13             Do you think abortion should be legal?   \n",
       "14             Do you think abortion should be legal?   \n",
       "15  We see from our side that maybe they are miser...   \n",
       "16  We see from our side that maybe they are miser...   \n",
       "17  We see from our side that maybe they are miser...   \n",
       "18  We see from our side that maybe they are miser...   \n",
       "19  We see from our side that maybe they are miser...   \n",
       "\n",
       "                                               answer  \\\n",
       "0   I am pro choice. I feel bad for the baby. Peop...   \n",
       "1   Logically, both make sense. Conflicted between...   \n",
       "2   I'm pro life. Because in my belief, if a fetus...   \n",
       "3   Prochoice. If she was a victim of rape, etc, s...   \n",
       "4   Prolife for religious reasons. Being religious...   \n",
       "5               Prochoice. Unless the baby is normal.   \n",
       "6   Legal - not really legal - legal for special c...   \n",
       "7   Agree with other solutions besides abortion - ...   \n",
       "8   Should be legal with criteria. Agree with Indo...   \n",
       "9         What counts as a person? Is fetus a person?   \n",
       "10  Does not being a Christian still fall under Ch...   \n",
       "11  If it has a heartbeat, can it be called life? ...   \n",
       "12  when something grows doesnt it count as life a...   \n",
       "13  KBBI - the process of baby child teenager adul...   \n",
       "14  I don't agree with Adam and Eve because not ev...   \n",
       "15  Everyone has rights, even fetuses or babies, b...   \n",
       "16  Is it wrong to regret life? That is the right ...   \n",
       "17  Actually, we humans are well aware that life i...   \n",
       "18  The law didn't need to be revised. Choosing th...   \n",
       "19  Rape victims, let's not confuse them, let's di...   \n",
       "\n",
       "                                       Document Embed  \n",
       "0   [-0.022362433, 0.01713121, 0.01336585, -0.0077...  \n",
       "1   [-0.007254202, 0.009839708, -0.00799253, -0.01...  \n",
       "2   [-0.014276878, 0.0022278614, -0.010473907, 0.0...  \n",
       "3   [-0.022501977, 0.024691759, 0.005002156, 0.017...  \n",
       "4   [-0.0040696473, -0.017911343, -0.02254271, 0.0...  \n",
       "5   [-0.098942615, 0.024454754, -0.070881665, 0.05...  \n",
       "6   [0.004587771, -0.0013291183, 0.0049727913, 0.0...  \n",
       "7   [-0.01381315, 0.01531402, 0.0022275664, -0.004...  \n",
       "8   [-0.009043147, 0.021918792, -0.030498233, -0.0...  \n",
       "9   [-0.22057891, 0.014038535, -0.20972323, -0.011...  \n",
       "10  [-0.10089697, -0.104009986, -0.051932067, 0.09...  \n",
       "11  [0.006337575, 0.01460853, -0.017543007, 0.0111...  \n",
       "12  [-0.052422117, 0.030819645, -0.032700323, -0.0...  \n",
       "13  [0.005943503, 0.010664682, -0.022690445, -0.01...  \n",
       "14  [0.016778132, 0.00512473, 0.023938367, -0.0485...  \n",
       "15  [0.008111107, 0.011857094, 0.019276416, -0.016...  \n",
       "16  [-0.004189881, 0.0068831732, -0.020556947, 0.0...  \n",
       "17  [-0.0025377192, 7.386828e-05, 0.006843238, -0....  \n",
       "18  [-0.0076232995, 0.0067849657, 0.038440254, 0.0...  \n",
       "19  [-0.010766402, 0.0011867925, -0.018306145, -0....  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [PreprocessDocument(doc, isLemma=True, isStopWords=True) for doc in df[\"answer\"]]\n",
    "word_embedded_docs = []\n",
    "for doc in docs:\n",
    "    word_embedded_docs.append(WordEmbed(doc, model))\n",
    "\n",
    "# sentence_embed(\"bababui\", tfidf_matrix=my_tfidf, index=1)\n",
    "doc_embeds = SentenceEmbedWeighted(word_embedded_docs, my_tfidf, \"avg\")\n",
    "df[\"Document Embed\"] = doc_embeds\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(inplace=True) # don't forget to add this after every row-altering operation.\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>LDA Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.015178566, 0.015176928, 0.015178298, 0.01517892, 0.015176928, 0.015178965, 0.015177403, 0.86339366, 0.015181374, 0.015178913]\n",
      "[0.8715595, 0.014267717, 0.014270598, 0.014271023, 0.014267717, 0.0142730465, 0.014269685, 0.014276249, 0.014271883, 0.014272554]\n",
      "[0.011866485, 0.011865227, 0.011866371, 0.011866245, 0.011865227, 0.011867544, 0.011865903, 0.011869738, 0.011866222, 0.8932011]\n",
      "[0.019781291, 0.01978034, 0.821959, 0.019786734, 0.01978034, 0.019783786, 0.019781467, 0.019782048, 0.0197826, 0.019782448]\n",
      "[0.014406338, 0.014404694, 0.014406862, 0.014408974, 0.014404694, 0.01441362, 0.8703269, 0.014411853, 0.014407115, 0.014409003]\n",
      "[0.030097462, 0.03009658, 0.03009934, 0.03010569, 0.03009658, 0.030103652, 0.030097155, 0.030099232, 0.72910494, 0.030099368]\n",
      "[0.015671797, 0.015671214, 0.015672458, 0.015671609, 0.015671214, 0.8589464, 0.015671996, 0.01567902, 0.015671827, 0.015672453]\n",
      "[0.018617284, 0.018616846, 0.01861919, 0.018619861, 0.018616846, 0.01862093, 0.01861727, 0.832436, 0.0186182, 0.018617587]\n",
      "[0.021005102, 0.021004882, 0.021005278, 0.021007098, 0.021004882, 0.02101101, 0.0210055, 0.8109441, 0.021006797, 0.021005388]\n",
      "[0.03092641, 0.03092524, 0.030927047, 0.030925797, 0.03092524, 0.03092602, 0.0309255, 0.7216643, 0.03092624, 0.030928174]\n",
      "[0.027838847, 0.027838647, 0.027838733, 0.027838647, 0.027838647, 0.027838837, 0.027838873, 0.7494513, 0.027838647, 0.02783889]\n",
      "[0.015906438, 0.015904922, 0.015906258, 0.015906079, 0.015904922, 0.015910769, 0.015906626, 0.015907886, 0.8568398, 0.015906291]\n",
      "[0.80662626, 0.021484952, 0.021485008, 0.02148516, 0.021484952, 0.021486562, 0.021485256, 0.02148918, 0.021486238, 0.021486444]\n",
      "[0.012575774, 0.012575189, 0.012578738, 0.012578993, 0.012575189, 0.012580878, 0.012576052, 0.17273933, 0.012580581, 0.7266393]\n",
      "[0.022880103, 0.02288003, 0.02288067, 0.022880796, 0.02288003, 0.022880593, 0.022880632, 0.7940745, 0.02288003, 0.022882586]\n",
      "[0.016989704, 0.016989578, 0.0169908, 0.01699051, 0.016989578, 0.84708554, 0.016989684, 0.016991306, 0.01699314, 0.01699014]\n",
      "[0.016180033, 0.016177343, 0.016180292, 0.01618138, 0.016177343, 0.8543803, 0.016177965, 0.016183859, 0.016179897, 0.016181605]\n",
      "[0.013030089, 0.013026951, 0.8827176, 0.013028377, 0.013026951, 0.013034541, 0.013029281, 0.013044222, 0.013030561, 0.013031416]\n",
      "[0.016423983, 0.01642277, 0.01642425, 0.8521783, 0.01642277, 0.01643073, 0.016423335, 0.016425284, 0.016424347, 0.016424261]\n",
      "[0.011440102, 0.011438417, 0.011446098, 0.89699805, 0.011438417, 0.011444645, 0.011440321, 0.011470491, 0.011440754, 0.01144269]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "input:\n",
    "- doclist : list<list<str>> --> list of tokenized sentences/docs\n",
    "- num_topics : number of inferred topics.\n",
    "'''\n",
    "'''\n",
    "output:\n",
    "- docFeatureList : list<list<float>> --> topic distribution for each sentence/doc\n",
    "'''\n",
    "def GetLDADistribution(doclist: list, topics: int = 5):\n",
    "    new_corpus = []\n",
    "    for i in range(len(docs)):\n",
    "        doc = [(j, my_matrix[i, j]) for j in my_matrix[i].indices]\n",
    "        new_corpus.append(doc)\n",
    "    gensim_dict = corpora.Dictionary.from_corpus(new_corpus)\n",
    "    lda_model = gensim.models.LdaModel(new_corpus, num_topics=10, id2word=gensim_dict)\n",
    "    goofy_ahh_doc_topic_distributions = lda_model[new_corpus]\n",
    "    docFeatureList = []\n",
    "    for doc_topic_dist in goofy_ahh_doc_topic_distributions:\n",
    "        featureList = [0.0 for i in range(0, 10)]\n",
    "        for topic_dist in doc_topic_dist:\n",
    "            featureList[topic_dist[0]] = topic_dist[1]\n",
    "        docFeatureList.append(featureList)\n",
    "    return docFeatureList\n",
    "\n",
    "# myTopicDist = GetLDADistribution(docs, 10)\n",
    "# for topicDist in myTopicDist:\n",
    "#     print(topicDist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Anomaly detection : DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "inputs:\n",
    "- vectors : list<list<float>> --> list of features corresponding to each doc/sentence\n",
    "- epsilon : float --> the radius within which points are considered connected.\n",
    "- min : int --> minimum amount of connected points for a point to be considered a core point of a cluster.\n",
    "'''\n",
    "'''\n",
    "output:\n",
    "clusters : list<int> --> a list of integers to assign each data point to a cluster. -1 means outlier.\n",
    "'''\n",
    "def GetDBSCANClusters(vectors, epsilon:float, min:int):\n",
    "    dbscan = DBSCAN(eps=epsilon, min_samples=min)\n",
    "    clusters = dbscan.fit_predict(vectors)\n",
    "    # plt.title(\"to the depths of depravity {} and the cusp of blasphemy {}.\".format(epsilon, min))\n",
    "    # plt.scatter(vectors[:, 0], vectors[:, 1], c=clusters)\n",
    "    # plt.show()\n",
    "    print(clusters)\n",
    "    return clusters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0  0  0 -1  0  0  0 -1 -1  0  0  0  0  0  0  0  0  0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0, -1,  0,  0,  0, -1, -1,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GetDBSCANClusters(list(df[\"Document Embed\"]), 1, 2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Shrink and draw with TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a tsne shrinkage also...\n",
    "def plot_documents(df:db.pd.DataFrame, isPrint=False):\n",
    "    labels = np.arange(0, df.index.stop, 1)\n",
    "    values = list(df[\"Document Embed\"]) # don't forget to list it first, then np array it later.\n",
    "\n",
    "    # train model\n",
    "    tsne_model = TSNE(perplexity=20, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(np.array(values))\n",
    "\n",
    "    # plot\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "    \n",
    "    if isPrint:\n",
    "        plt.figure(figsize=(20, 20)) \n",
    "        for i in range(len(x)):\n",
    "            plt.scatter(x[i],y[i])\n",
    "            plt.annotate(labels[i],\n",
    "                        xy=(x[i], y[i]),\n",
    "                        xytext=(5, 2),\n",
    "                        textcoords='offset points',\n",
    "                        ha='right',\n",
    "                        va='bottom')\n",
    "        plt.show()\n",
    "    # use the thing to find new clusters.\n",
    "    return new_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>event_title</th>\n",
       "      <th>speaker</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>Document Embed</th>\n",
       "      <th>Cluster Assignment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>230</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>KSMG</td>\n",
       "      <td>Pro life or pro choice?</td>\n",
       "      <td>I am pro choice. I feel bad for the baby. Peop...</td>\n",
       "      <td>[-0.022362433, 0.01713121, 0.01336585, -0.0077...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>231</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>JJ</td>\n",
       "      <td>Pro life or pro choice?</td>\n",
       "      <td>Logically, both make sense. Conflicted between...</td>\n",
       "      <td>[-0.007254202, 0.009839708, -0.00799253, -0.01...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>232</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>JER</td>\n",
       "      <td>Pro life or pro choice?</td>\n",
       "      <td>I'm pro life. Because in my belief, if a fetus...</td>\n",
       "      <td>[-0.014276878, 0.0022278614, -0.010473907, 0.0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>233</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>YOT</td>\n",
       "      <td>Pro life or pro choice?</td>\n",
       "      <td>Prochoice. If she was a victim of rape, etc, s...</td>\n",
       "      <td>[-0.022501977, 0.024691759, 0.005002156, 0.017...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>234</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>GRE</td>\n",
       "      <td>Pro life or pro choice?</td>\n",
       "      <td>Prolife for religious reasons. Being religious...</td>\n",
       "      <td>[-0.0040696473, -0.017911343, -0.02254271, 0.0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>236</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>GRE</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>Legal - not really legal - legal for special c...</td>\n",
       "      <td>[0.004587771, -0.0013291183, 0.0049727913, 0.0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>237</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>MAR</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>Agree with other solutions besides abortion - ...</td>\n",
       "      <td>[-0.01381315, 0.01531402, 0.0022275664, -0.004...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>238</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>RIC</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>Should be legal with criteria. Agree with Indo...</td>\n",
       "      <td>[-0.009043147, 0.021918792, -0.030498233, -0.0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>241</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>YOT</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>If it has a heartbeat, can it be called life? ...</td>\n",
       "      <td>[0.006337575, 0.01460853, -0.017543007, 0.0111...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>242</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>JJ</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>when something grows doesnt it count as life a...</td>\n",
       "      <td>[-0.052422117, 0.030819645, -0.032700323, -0.0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>243</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>STN</td>\n",
       "      <td>Do you think abortion should be legal?</td>\n",
       "      <td>KBBI - the process of baby child teenager adul...</td>\n",
       "      <td>[0.005943503, 0.010664682, -0.022690445, -0.01...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>245</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>STN</td>\n",
       "      <td>We see from our side that maybe they are miser...</td>\n",
       "      <td>Everyone has rights, even fetuses or babies, b...</td>\n",
       "      <td>[0.008111107, 0.011857094, 0.019276416, -0.016...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>246</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>GRE</td>\n",
       "      <td>We see from our side that maybe they are miser...</td>\n",
       "      <td>Is it wrong to regret life? That is the right ...</td>\n",
       "      <td>[-0.004189881, 0.0068831732, -0.020556947, 0.0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>247</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>YOT</td>\n",
       "      <td>We see from our side that maybe they are miser...</td>\n",
       "      <td>Actually, we humans are well aware that life i...</td>\n",
       "      <td>[-0.0025377192, 7.386828e-05, 0.006843238, -0....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>248</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>STN</td>\n",
       "      <td>We see from our side that maybe they are miser...</td>\n",
       "      <td>The law didn't need to be revised. Choosing th...</td>\n",
       "      <td>[-0.0076232995, 0.0067849657, 0.038440254, 0.0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>249</td>\n",
       "      <td>Of Choice and Life</td>\n",
       "      <td>YOT</td>\n",
       "      <td>We see from our side that maybe they are miser...</td>\n",
       "      <td>Rape victims, let's not confuse them, let's di...</td>\n",
       "      <td>[-0.010766402, 0.0011867925, -0.018306145, -0....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index   id         event_title speaker  \\\n",
       "0       0  230  Of Choice and Life    KSMG   \n",
       "1       1  231  Of Choice and Life      JJ   \n",
       "2       2  232  Of Choice and Life     JER   \n",
       "3       3  233  Of Choice and Life     YOT   \n",
       "4       4  234  Of Choice and Life     GRE   \n",
       "6       6  236  Of Choice and Life     GRE   \n",
       "7       7  237  Of Choice and Life     MAR   \n",
       "8       8  238  Of Choice and Life     RIC   \n",
       "11     11  241  Of Choice and Life     YOT   \n",
       "12     12  242  Of Choice and Life      JJ   \n",
       "13     13  243  Of Choice and Life     STN   \n",
       "15     15  245  Of Choice and Life     STN   \n",
       "16     16  246  Of Choice and Life     GRE   \n",
       "17     17  247  Of Choice and Life     YOT   \n",
       "18     18  248  Of Choice and Life     STN   \n",
       "19     19  249  Of Choice and Life     YOT   \n",
       "\n",
       "                                             question  \\\n",
       "0                             Pro life or pro choice?   \n",
       "1                             Pro life or pro choice?   \n",
       "2                             Pro life or pro choice?   \n",
       "3                             Pro life or pro choice?   \n",
       "4                             Pro life or pro choice?   \n",
       "6              Do you think abortion should be legal?   \n",
       "7              Do you think abortion should be legal?   \n",
       "8              Do you think abortion should be legal?   \n",
       "11             Do you think abortion should be legal?   \n",
       "12             Do you think abortion should be legal?   \n",
       "13             Do you think abortion should be legal?   \n",
       "15  We see from our side that maybe they are miser...   \n",
       "16  We see from our side that maybe they are miser...   \n",
       "17  We see from our side that maybe they are miser...   \n",
       "18  We see from our side that maybe they are miser...   \n",
       "19  We see from our side that maybe they are miser...   \n",
       "\n",
       "                                               answer  \\\n",
       "0   I am pro choice. I feel bad for the baby. Peop...   \n",
       "1   Logically, both make sense. Conflicted between...   \n",
       "2   I'm pro life. Because in my belief, if a fetus...   \n",
       "3   Prochoice. If she was a victim of rape, etc, s...   \n",
       "4   Prolife for religious reasons. Being religious...   \n",
       "6   Legal - not really legal - legal for special c...   \n",
       "7   Agree with other solutions besides abortion - ...   \n",
       "8   Should be legal with criteria. Agree with Indo...   \n",
       "11  If it has a heartbeat, can it be called life? ...   \n",
       "12  when something grows doesnt it count as life a...   \n",
       "13  KBBI - the process of baby child teenager adul...   \n",
       "15  Everyone has rights, even fetuses or babies, b...   \n",
       "16  Is it wrong to regret life? That is the right ...   \n",
       "17  Actually, we humans are well aware that life i...   \n",
       "18  The law didn't need to be revised. Choosing th...   \n",
       "19  Rape victims, let's not confuse them, let's di...   \n",
       "\n",
       "                                       Document Embed  Cluster Assignment  \n",
       "0   [-0.022362433, 0.01713121, 0.01336585, -0.0077...                   0  \n",
       "1   [-0.007254202, 0.009839708, -0.00799253, -0.01...                   0  \n",
       "2   [-0.014276878, 0.0022278614, -0.010473907, 0.0...                   0  \n",
       "3   [-0.022501977, 0.024691759, 0.005002156, 0.017...                   0  \n",
       "4   [-0.0040696473, -0.017911343, -0.02254271, 0.0...                   0  \n",
       "6   [0.004587771, -0.0013291183, 0.0049727913, 0.0...                   0  \n",
       "7   [-0.01381315, 0.01531402, 0.0022275664, -0.004...                   0  \n",
       "8   [-0.009043147, 0.021918792, -0.030498233, -0.0...                   0  \n",
       "11  [0.006337575, 0.01460853, -0.017543007, 0.0111...                   0  \n",
       "12  [-0.052422117, 0.030819645, -0.032700323, -0.0...                   0  \n",
       "13  [0.005943503, 0.010664682, -0.022690445, -0.01...                   0  \n",
       "15  [0.008111107, 0.011857094, 0.019276416, -0.016...                   0  \n",
       "16  [-0.004189881, 0.0068831732, -0.020556947, 0.0...                   0  \n",
       "17  [-0.0025377192, 7.386828e-05, 0.006843238, -0....                   0  \n",
       "18  [-0.0076232995, 0.0067849657, 0.038440254, 0.0...                   0  \n",
       "19  [-0.010766402, 0.0011867925, -0.018306145, -0....                   0  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clusters = GetDBSCANClusters(list(df[\"Document Embed\"]), 0.6, 5)\n",
    "# df[\"Cluster Assignment\"] = clusters\n",
    "# dfSupposedOutliers = df.loc[df[\"Cluster Assignment\"] == -1]\n",
    "# dfSupposedGoods = df.loc[df[\"Cluster Assignment\"] != -1]\n",
    "# dfSupposedGoods.reset_index(inplace=True)\n",
    "# dfSupposedOutliers.reset_index(inplace=True)\n",
    "\n",
    "# for i in range(len(dfSupposedOutliers.index)):\n",
    "#     print(dfSupposedOutliers.loc[i][\"question\"], \" | \", dfSupposedOutliers.loc[i][\"answer\"])\n",
    "\n",
    "# print(\"#\" * 80)\n",
    "# for i in range(len(dfSupposedGoods.index)):\n",
    "#     print(dfSupposedGoods.loc[i][\"question\"], \" | \", dfSupposedGoods.loc[i][\"answer\"])\n",
    "\n",
    "'''\n",
    "inputs :\n",
    "- clusters : list<int> --> a list of clusters assigned to each doc/sentence\n",
    "- df : DataFrame --> the dataframe in question\n",
    "'''\n",
    "'''\n",
    "outputs:\n",
    "- dfOutliers : DataFrame --> the dataframe whose answers have been marked as outliers.\n",
    "- dfGoods : DataFrame --> the dataframe whose answers have not been marked as outliers.\n",
    "'''\n",
    "def ReturnClusters(clusters:list, df:db.pd.DataFrame):\n",
    "    df[\"Cluster Assignment\"] = clusters\n",
    "    dfGoods = df.loc[df[\"Cluster Assignment\"] != -1]\n",
    "    dfOutliers = df.loc[df[\"Cluster Assignment\"] == -1]\n",
    "    return dfOutliers, dfGoods\n",
    "\n",
    "# outliers, goods = ReturnClusters(clusters, df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Anomaly detection : LOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetLDADistribution(doclist: list, topics: int = 5, use_tfidf: bool = True):\n",
    "    new_corpus = []\n",
    "    \n",
    "    if use_tfidf:\n",
    "        for i in range(len(doclist)):\n",
    "            doc = [(j, my_matrix[i, j]) for j in my_matrix[i].indices]\n",
    "            new_corpus.append(doc)\n",
    "            gensim_dict = corpora.Dictionary.from_corpus(new_corpus)        \n",
    "    else:\n",
    "        gensim_dict = corpora.Dictionary(doclist)\n",
    "        new_corpus = [gensim_dict.doc2bow(doc) for doc in doclist]\n",
    "        \n",
    "    lda_model = gensim.models.LdaModel(new_corpus, num_topics=topics, id2word=gensim_dict)\n",
    "    goofy_ahh_doc_topic_distributions = lda_model[new_corpus]\n",
    "    \n",
    "    docFeatureList = []\n",
    "    for doc_topic_dist in goofy_ahh_doc_topic_distributions:\n",
    "        featureList = [0.0 for i in range(0, topics)]\n",
    "        for topic_dist in doc_topic_dist:\n",
    "            featureList[topic_dist[0]] = topic_dist[1]\n",
    "        docFeatureList.append(featureList)\n",
    "    \n",
    "    return docFeatureList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9594759, 0.040524174]\n",
      "[0.97850245, 0.021497536]\n",
      "[0.28016606, 0.7198339]\n",
      "[0.043442782, 0.9565572]\n",
      "[0.9731329, 0.026867067]\n",
      "[0.82650346, 0.17349651]\n",
      "[0.023954507, 0.9760455]\n",
      "[0.92979133, 0.07020863]\n",
      "[0.34196287, 0.6580371]\n",
      "[0.123561874, 0.87643814]\n",
      "[0.09156124, 0.90843874]\n",
      "[0.030009115, 0.9699909]\n",
      "[0.052324902, 0.94767505]\n",
      "[0.015818512, 0.98418146]\n",
      "[0.906932, 0.09306796]\n",
      "[0.03421068, 0.96578926]\n",
      "[0.02900701, 0.970993]\n",
      "[0.018301684, 0.98169833]\n",
      "[0.97891814, 0.021081883]\n",
      "[0.9852539, 0.014746079]\n"
     ]
    }
   ],
   "source": [
    "thing = GetLDADistribution(docs, 2, False)\n",
    "for item in thing:\n",
    "    print(item)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Anomaly detection : Isolation Forest (sklearn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Final Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyDetector():\n",
    "    def __init__(self, dbName: str = \"\",  dh=None, model=None, modelName=\"glove-wiki-gigaword-300\") -> None:\n",
    "        if dh is None:\n",
    "            self.dh = db.DatabaseHandler(dbName=dbName)\n",
    "        else:\n",
    "            self.dh = dh\n",
    "        if model is None:\n",
    "            self.model = api.load(modelName)\n",
    "        else:\n",
    "            self.model = model\n",
    "\n",
    "    '''\n",
    "    inputs :\n",
    "    - dh : DatabaseHandler --> to retrieve data from database\n",
    "    - eventID : int --> we're doing this by event, so straight to the eventID\n",
    "    - selector : str --> pretty much formality.\n",
    "    - splitBySentences : bool --> Split each doc into sentences or not. Defaults to no.\n",
    "    '''\n",
    "    '''\n",
    "    outputs:\n",
    "    None, just setting\n",
    "    '''\n",
    "\n",
    "    def SetDF(self, dh: db.DatabaseHandler, eventID: int, selector: str = \"event_id\", splitBySentences: bool = False):\n",
    "        self.df = self.dh.get_recordDataJoinedDF(selector=selector, ID=eventID)\n",
    "        if splitBySentences:\n",
    "            # df.set_index('id', inplace=True)\n",
    "            self.df['answer'] = self.df['answer'].str.split('.')\n",
    "            self.df = self.df.explode(\"answer\", True)\n",
    "            self.df.drop(self.df[self.df[\"answer\"] == \"\"].index, inplace=True)\n",
    "            self.df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    '''\n",
    "    inputs :\n",
    "    - dh : DatabaseHandler --> to retrieve data from database\n",
    "    - eventID : int --> we're doing this by event, so straight to the eventID\n",
    "    - selector : str --> pretty much formality.\n",
    "    - splitBySentences : bool --> Split each doc into sentences or not. Defaults to no.\n",
    "    '''\n",
    "    '''\n",
    "    outputs:\n",
    "    - df : DataFrame --> dataframe containing the thing we're gonna be using.\n",
    "    '''\n",
    "\n",
    "    def GetDF(self, dh: db.DatabaseHandler, eventID: int, selector: str = \"event_id\", splitBySentences: bool = False):\n",
    "        df = dh.get_recordDataJoinedDF(selector=selector, ID=eventID)\n",
    "        if splitBySentences:\n",
    "            # df.set_index('id', inplace=True)\n",
    "            df['answer'] = df['answer'].str.split('.')\n",
    "            df = df.explode(\"answer\", True)\n",
    "            df.drop(df[df[\"answer\"] == \"\"].index, inplace=True)\n",
    "            df.reset_index(drop=True, inplace=True)\n",
    "        return df\n",
    "\n",
    "    '''\n",
    "    inputs:\n",
    "    - doc : str --> a string representing a sentence/document.\n",
    "    - isLemma : bool --> use lemmatizer or not? Defaults to not.\n",
    "    - isStopWords : bool --> use stopwords or not? Defaults to not.\n",
    "    - isInflect : bool --> use inflections (you're --> you are) or not? Defaults to not.\n",
    "    - isNumberFiltered :  bool --> delete numbers in the string? Defaults to yes. \n",
    "    '''\n",
    "    '''\n",
    "    output : list<str> --> a list of word tokens (list<string>)\n",
    "    '''\n",
    "\n",
    "    def PreprocessDocument(self, doc: str, isLemma: bool = False, isStopWords: bool = False, isInflect: bool = False, isNumberFiltered: bool = True):\n",
    "        inflector = inflect.engine()\n",
    "        stopwordSet = set(stopwords.words(\"english\"))\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        punctuations = string.punctuation\n",
    "        # if numbers are filtered, add that to the punctuation string\n",
    "        if isNumberFiltered:\n",
    "            punctuations += \"1234567890\"\n",
    "\n",
    "        # case fold\n",
    "        doc = doc.lower()\n",
    "\n",
    "        # remove puncs\n",
    "        doc = \"\".join([char for char in doc if char not in punctuations])\n",
    "\n",
    "        # tokenize it.\n",
    "        token_list = nltk.word_tokenize(doc)\n",
    "\n",
    "        for i in range(len(token_list)):\n",
    "            # if inflect\n",
    "            if isInflect:\n",
    "                if token_list[i].isdigit():\n",
    "                    token_list[i] = inflector.number_to_words(token_list[i])\n",
    "\n",
    "            # if lemma\n",
    "            if isLemma:\n",
    "                tagged_word = nltk.pos_tag([token_list[i]])\n",
    "                wordnet_pos = self.getWordnetPos(tagged_word[0][1])\n",
    "                token_list[i] = lemmatizer.lemmatize(\n",
    "                    tagged_word[0][0], pos=wordnet_pos)\n",
    "\n",
    "            # if stopword\n",
    "            if isStopWords:\n",
    "                if token_list[i] in stopwordSet or token_list[i].isdigit():\n",
    "                    token_list[i] = \"#\"  # mark as #\n",
    "\n",
    "        # remove the marked strings\n",
    "        token_list = [token for token in token_list if token != \"#\"]\n",
    "\n",
    "        if token_list:\n",
    "            return token_list\n",
    "        return [\"\"]\n",
    "\n",
    "    '''\n",
    "    inputs:\n",
    "    - tag : str --> the tag obtained from POS tagging.\n",
    "    '''\n",
    "    '''\n",
    "    outputs:\n",
    "    - str --> Wordnet POS tag.\n",
    "    '''\n",
    "\n",
    "    def getWordnetPos(self, tag):\n",
    "        \"\"\"Map POS tag to WordNet POS tag\"\"\"\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN  # solves as noun by default.\n",
    "\n",
    "    '''\n",
    "    inputs:\n",
    "    - doclist : list<str> --> list of doc/sentences.\n",
    "    - isProcessed : bool --> has it already been preprocessed? Defaults to True.\n",
    "    '''\n",
    "    '''\n",
    "    outputs:\n",
    "    - df_tfidf : Dataframe --> the TFIDF matrix in df form. \n",
    "    - matrix : matrix --> the TFIDF matrix purely. mainly for LDA purposes.\n",
    "    '''\n",
    "\n",
    "    def GetTFIDF(self, doclist: list, isPreprocessed=True):\n",
    "        if not isPreprocessed:\n",
    "            doclist = [self.PreprocessDocument(\n",
    "                doc, isLemma=True, isStopWords=True) for doc in doclist]\n",
    "        # else:\n",
    "        #     # just tokenize the thing\n",
    "        #     doclist = [nltk.word_tokenize(doc) for doc in doclist]\n",
    "        # i think the thing has already been tokenized. That's the problem.\n",
    "        flat_doclist = [' '.join(doc)\n",
    "                        for doc in doclist]  # turn into one big corpus\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        matrix = vectorizer.fit_transform(flat_doclist)\n",
    "        tfidf_keys = vectorizer.get_feature_names_out()\n",
    "        df_tfidf = db.pd.DataFrame(matrix.toarray(), columns=tfidf_keys)\n",
    "\n",
    "        return df_tfidf, matrix\n",
    "\n",
    "    # input : list<str> : tokens of one document/sentence\n",
    "    # output : list<(str, list<int>[300])> : list of word-vector pair for each word available on the model\n",
    "    def WordEmbed(self, document: list, model):\n",
    "        word_embed_pairs = []\n",
    "        for word in document:\n",
    "            if word in model:\n",
    "                word_embed_pairs.append((word, model[word]))\n",
    "        return word_embed_pairs\n",
    "\n",
    "    # input : list<(str, list<float>[300])>, str : word-vector pair list and preferred agg method.\n",
    "    # output : list<float>[300] : 300-d vector that represents an aggregated value of the input words\n",
    "    def SentenceEmbedUnweightedFunction(self, word_embed_pair_list: list, aggregateMethod: str = \"avg\"):\n",
    "        wvs = []\n",
    "        for pair in word_embed_pair_list:\n",
    "            wvs.append(pair[1])\n",
    "        if aggregateMethod == \"avg\":\n",
    "            return np.mean(wvs, axis=0)\n",
    "        else:\n",
    "            return np.sum(wvs, axis=0)\n",
    "\n",
    "    # input : list<list<(str, list<float>[300])>>, str : list containing word-vector pairs and preferred agg method\n",
    "    # output : list<(str, list<int>[300])> : list containing sentence-vector pairs.\n",
    "    def SentenceEmbedUnweighted(self, word_embedded_docs: list, aggregateMethod: str = \"avg\"):\n",
    "        sentence_embedded_docs = []\n",
    "        for i in range(len(word_embedded_docs)):\n",
    "            sentence_embedded_docs.append(self.SentenceEmbedUnweightedFunction(\n",
    "                word_embedded_docs[i], aggregateMethod))\n",
    "        return sentence_embedded_docs\n",
    "\n",
    "    '''\n",
    "    input :\n",
    "    list<list<(str, list<float>[300])>> : word-vector pair list\n",
    "    matrix : tf-idf matrix for the corresponding doc\n",
    "    int : the row we want\n",
    "    str : preferred agg method\n",
    "    '''\n",
    "    # output : list<float>[300] : 300-d vector that represents an aggregated value of the input words\n",
    "\n",
    "    def SentenceEmbedWeightedFunction(self, word_embed_pair_list: list, tfidf_matrix, index: int, aggregateMethod: str = \"avg\"):\n",
    "        weighted_wvs = []\n",
    "        # multiplies each word with its TF-IDF value in the corresponding row. Is 0 if word isn't found somehow.\n",
    "        for pair in word_embed_pair_list:\n",
    "            tfidf_weight = 0\n",
    "            if pair[0] in tfidf_matrix:\n",
    "                tfidf_weight = tfidf_matrix[pair[0]][index]\n",
    "            weighted_wvs.append(pair[1] * tfidf_weight)\n",
    "        # turn into array for fast aggregating\n",
    "        weighted_wvs = np.array(weighted_wvs)\n",
    "        if aggregateMethod == \"avg\":\n",
    "            sentence_vector = np.mean(weighted_wvs, axis=0)\n",
    "        else:\n",
    "            sentence_vector = np.sum(weighted_wvs, axis=0)\n",
    "        return sentence_vector\n",
    "\n",
    "    # input : list<list<(str, list<float>[300])>>, str : list containing word-vector pairs, TF-IDF matrix of the corpus, and preferred agg method\n",
    "    # output : list<(str, list<float>[300])> : list containing sentence-vector pairs.\n",
    "    def SentenceEmbedWeighted(self, word_embedded_docs: list, tfidf_matrix, aggregateMethod=\"avg\"):\n",
    "        sentence_embedded_docs = []\n",
    "        for i in range(len(word_embedded_docs)):\n",
    "            sentence_embedded_docs.append(self.SentenceEmbedWeightedFunction(\n",
    "                word_embedded_docs[i], tfidf_matrix, i, aggregateMethod))\n",
    "        return sentence_embedded_docs\n",
    "\n",
    "    '''\n",
    "    input:\n",
    "    - doclist : list<list<str>> --> list of tokenized sentences/docs\n",
    "    - topics : int --> number of inferred topics.\n",
    "    - use_tfidf : bool --> use TFIDF or not? defaults to yes.\n",
    "    '''\n",
    "    '''\n",
    "    output:\n",
    "    - docFeatureList : list<list<float>> --> topic distribution for each sentence/doc\n",
    "    '''\n",
    "\n",
    "    def GetLDADistribution(self, doclist: list, topics: int = 5, use_tfidf: bool = True):\n",
    "        new_corpus = []\n",
    "\n",
    "        if use_tfidf:\n",
    "            for i in range(len(doclist)):\n",
    "                doc = [(j, self.tfidf_matrix[i, j])\n",
    "                       for j in self.tfidf_matrix[i].indices]\n",
    "                new_corpus.append(doc)\n",
    "                gensim_dict = corpora.Dictionary.from_corpus(new_corpus)\n",
    "        else:\n",
    "            gensim_dict = corpora.Dictionary(doclist)\n",
    "            new_corpus = [gensim_dict.doc2bow(doc) for doc in doclist]\n",
    "\n",
    "        lda_model = gensim.models.LdaModel(\n",
    "            new_corpus, num_topics=topics, id2word=gensim_dict)\n",
    "        goofy_ahh_doc_topic_distributions = lda_model[new_corpus]\n",
    "\n",
    "        docFeatureList = []\n",
    "        for doc_topic_dist in goofy_ahh_doc_topic_distributions:\n",
    "            featureList = [0.0 for i in range(0, topics)]\n",
    "            for topic_dist in doc_topic_dist:\n",
    "                featureList[topic_dist[0]] = topic_dist[1]\n",
    "            docFeatureList.append(featureList)\n",
    "\n",
    "        return docFeatureList\n",
    "\n",
    "    '''\n",
    "    inputs:\n",
    "    - vectors : list<list<float>> --> list of features corresponding to each doc/sentence\n",
    "    - epsilon : float --> the radius within which points are considered connected.\n",
    "    - min : int --> minimum amount of connected points for a point to be considered a core point of a cluster.\n",
    "    '''\n",
    "    '''\n",
    "    output:\n",
    "    clusters : list<int> --> a list of integers to assign each data point to a cluster. -1 means outlier.\n",
    "    '''\n",
    "\n",
    "    def GetDBSCANClusters(self, vectors, epsilon: float, min: int):\n",
    "        dbscan = DBSCAN(eps=epsilon, min_samples=min)\n",
    "        clusters = dbscan.fit_predict(vectors)\n",
    "        # plt.title(\"to the depths of depravity {} and the cusp of blasphemy {}.\".format(epsilon, min))\n",
    "        # plt.scatter(vectors[:, 0], vectors[:, 1], c=clusters)\n",
    "        # plt.show()\n",
    "        # print(clusters)\n",
    "        return clusters\n",
    "\n",
    "    '''\n",
    "    inputs :\n",
    "    - clusters : list<int> --> a list of clusters assigned to each doc/sentence\n",
    "    - df : DataFrame --> the dataframe in question\n",
    "    '''\n",
    "    '''\n",
    "    outputs:\n",
    "    - dfOutliers : DataFrame --> the dataframe whose answers have been marked as outliers.\n",
    "    - dfGoods : DataFrame --> the dataframe whose answers have not been marked as outliers.\n",
    "    '''\n",
    "\n",
    "    def ReturnClusters(self, clusters: list, df: db.pd.DataFrame):\n",
    "        df[\"Cluster Assignment\"] = clusters\n",
    "        dfGoods = df.loc[df[\"Cluster Assignment\"] != -1]\n",
    "        dfOutliers = df.loc[df[\"Cluster Assignment\"] == -1]\n",
    "        return dfOutliers, dfGoods\n",
    "\n",
    "    def GetAnomalies_DBSCAN_Embedding(self, isWeighted: bool = True, aggregateMethod: str = \"avg\", epsilon: float = 0.01, minsamp: int = 2):\n",
    "        # df and model are obtained by invoking a separate function, and it is assumed to be already available when invoking this function.\n",
    "\n",
    "        # preprocess each doc/sentence\n",
    "        self.preprocessedDocs = [self.PreprocessDocument(\n",
    "            doc, isLemma=True, isStopWords=True) for doc in self.df[\"answer\"]]\n",
    "\n",
    "        # extract feature with embedding\n",
    "        self.wordEmbeddedDocs = [self.WordEmbed(\n",
    "            doc, self.model) for doc in self.preprocessedDocs]\n",
    "\n",
    "        # if weighted, prepare TF-IDF and embed sentences with weight.\n",
    "        if isWeighted:\n",
    "            self.tfidf_df, self.tfidf_matrix = self.GetTFIDF(\n",
    "                self.preprocessedDocs)\n",
    "            self.doc_embeds = self.SentenceEmbedWeighted(\n",
    "                self.wordEmbeddedDocs, self.tfidf_df, aggregateMethod)\n",
    "        else:\n",
    "            self.doc_embeds = self.SentenceEmbedUnweighted(\n",
    "                self.wordEmbeddedDocs, aggregateMethod)\n",
    "\n",
    "        # append embedding to each document\n",
    "        if self.doc_embeds:\n",
    "            self.df[\"Document Embed\"] = self.doc_embeds\n",
    "            self.df = self.df.dropna(subset=[\"Document Embed\"]) # preventing NaN in the simplest fucking way in know.\n",
    "\n",
    "        # apply DBSCAN\n",
    "        self.clusters = self.GetDBSCANClusters(\n",
    "            list(self.df[\"Document Embed\"]), epsilon, minsamp)\n",
    "\n",
    "        # return the dfs\n",
    "        return self.ReturnClusters(self.clusters, self.df)\n",
    "\n",
    "    def GetAnomalies_DBSCAN_LDA(self, isWeighted: bool = True, topics: int = 5, epsilon: float = 0.01, minsamp: int = 5):\n",
    "        # df and model are obtained by invoking a separate function, and it is assumed to be already available when invoking this function.\n",
    "\n",
    "        # preprocess each doc/sentence\n",
    "        self.preprocessedDocs = [self.PreprocessDocument(\n",
    "            doc, isLemma=True, isStopWords=True) for doc in self.df[\"answer\"]]\n",
    "\n",
    "        # if weighted, prepare tf-idf matrix.\n",
    "        if isWeighted:\n",
    "            self.tfidf_df, self.tfidf_matrix = self.GetTFIDF(\n",
    "                self.preprocessedDocs)\n",
    "\n",
    "        # use the in-house options for weighted or not.\n",
    "        self.doc_embeds = self.GetLDADistribution(\n",
    "            self.preprocessedDocs, topics=topics, use_tfidf=isWeighted)\n",
    "\n",
    "        # append embedding to each document\n",
    "        if self.doc_embeds:\n",
    "            self.df[\"Document Embed\"] = self.doc_embeds\n",
    "            self.df = self.df.dropna(subset=[\"Document Embed\"]) # preventing NaN in the simplest fucking way in know.\n",
    "\n",
    "        # apply DBSCAN\n",
    "        self.clusters = self.GetDBSCANClusters(\n",
    "            list(self.df[\"Document Embed\"]), epsilon, minsamp)\n",
    "\n",
    "        # return the dfs\n",
    "        return self.ReturnClusters(self.clusters, self.df)\n",
    "\n",
    "    def GetAnomalies(self, method: str, model, isWeighted: bool = True, aggregateMethod: str = \"avg\", epsilon=0.01, minsamp=2, topics=5):\n",
    "        # initialize\n",
    "        # extract the dataset\n",
    "        self.df = self.GetDF()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad = AnomalyDetector(dbName=\"testdb.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad.SetDF(ad.dh, 18, \"event_id\", splitBySentences=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\peter\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\peter\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\core\\_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "myOutliers, myGoods = ad.GetAnomalies_DBSCAN_Embedding(isWeighted=True, aggregateMethod=\"avg\", epsilon=1.3, minsamp=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\AppData\\Local\\Temp\\ipykernel_22100\\256127518.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfTotal = myGoods.append(myOutliers)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_title</th>\n",
       "      <th>speaker</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>Document Embed</th>\n",
       "      <th>Cluster Assignment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>Why Did God Create Us?</td>\n",
       "      <td>PY</td>\n",
       "      <td>How did we come to exist?</td>\n",
       "      <td>Everybody should believe that they are creatio...</td>\n",
       "      <td>[0.006212553, 0.038233534, -0.024160838, -0.10...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>Why Did God Create Us?</td>\n",
       "      <td>GRE</td>\n",
       "      <td>How did we come to exist?</td>\n",
       "      <td>This is undeniable, and the Charles Darwin's ...</td>\n",
       "      <td>[-0.007556731, -0.009657541, 0.0465072, -0.032...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>Why Did God Create Us?</td>\n",
       "      <td>PY</td>\n",
       "      <td>Do you think you need to exist?</td>\n",
       "      <td>We don't really need to exist</td>\n",
       "      <td>[-0.11323776, -0.015670797, -0.079038456, 0.09...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Why Did God Create Us?</td>\n",
       "      <td>JJ</td>\n",
       "      <td>Do you think you need to exist?</td>\n",
       "      <td>I don't think we need to exist</td>\n",
       "      <td>[-0.1383083, -0.005404059, -0.09285827, 0.1340...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>Why Did God Create Us?</td>\n",
       "      <td>TMS</td>\n",
       "      <td>Do you think you need to exist?</td>\n",
       "      <td>I think we are a part of a bigger picture</td>\n",
       "      <td>[-0.0036777146, 0.0565521, -0.049426157, -0.08...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>Why Did God Create Us?</td>\n",
       "      <td>TMS</td>\n",
       "      <td>Do you think you need to exist?</td>\n",
       "      <td>Yes, because we're a part of a community where...</td>\n",
       "      <td>[-0.0068717278, -0.053223114, -0.028907582, -0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>Why Did God Create Us?</td>\n",
       "      <td>DJ</td>\n",
       "      <td>Do you think you need to exist?</td>\n",
       "      <td>In general,  humans have made many impacts</td>\n",
       "      <td>[-0.0020462424, 0.08268591, -0.013377609, -0.1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>Why Did God Create Us?</td>\n",
       "      <td>RIC</td>\n",
       "      <td>Do you think you need to exist?</td>\n",
       "      <td>This got me thinking, if we question the purpo...</td>\n",
       "      <td>[-0.061571285, -0.009830134, -0.029011544, -0....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>Why Did God Create Us?</td>\n",
       "      <td>GRE</td>\n",
       "      <td>From a bigger perspective, why did God create us?</td>\n",
       "      <td>It's a mystery</td>\n",
       "      <td>[-0.17551, -0.082361, 0.35437, 0.057444, 0.627...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>Why Did God Create Us?</td>\n",
       "      <td>TMS</td>\n",
       "      <td>From a bigger perspective, why did God create us?</td>\n",
       "      <td>I think God himself needs an outlet to His lov...</td>\n",
       "      <td>[-0.05835578, -0.04148901, -0.0784467, -0.0107...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>Why Did God Create Us?</td>\n",
       "      <td>RIC</td>\n",
       "      <td>From a bigger perspective, why did God create us?</td>\n",
       "      <td>God created us to glorify Him</td>\n",
       "      <td>[0.022100516, 0.03634443, -0.17097521, -0.0331...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>Why Did God Create Us?</td>\n",
       "      <td>PAU</td>\n",
       "      <td>From a bigger perspective, why did God create us?</td>\n",
       "      <td>God is full of mysteries</td>\n",
       "      <td>[-0.120145984, -0.08685847, 0.027744427, -0.06...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>Why Did God Create Us?</td>\n",
       "      <td>PAU</td>\n",
       "      <td>From a bigger perspective, why did God create us?</td>\n",
       "      <td>I too believe that humans are created for a pu...</td>\n",
       "      <td>[-0.07333655, 0.079368055, -0.15811524, -0.111...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>Why Did God Create Us?</td>\n",
       "      <td>LIZ</td>\n",
       "      <td>From a bigger perspective, why did God create us?</td>\n",
       "      <td>Maybe because God is a relational being, so He...</td>\n",
       "      <td>[-0.16157854, -0.012918956, -0.015164363, -0.0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>Why Did God Create Us?</td>\n",
       "      <td>RIC</td>\n",
       "      <td>From a bigger perspective, why did God create us?</td>\n",
       "      <td>I reckon God created humans so the world would...</td>\n",
       "      <td>[-0.004739574, 0.11398921, -0.101472095, -0.05...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>Why Did God Create Us?</td>\n",
       "      <td>AMD</td>\n",
       "      <td>Does God need to be glorified?</td>\n",
       "      <td>if we are indeed created to glorify Him, does ...</td>\n",
       "      <td>[0.042036586, 0.09319516, -0.1104986, 0.015954...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>Why Did God Create Us?</td>\n",
       "      <td>TMS</td>\n",
       "      <td>Does God need to be glorified?</td>\n",
       "      <td>i think if that's the only reason for humans t...</td>\n",
       "      <td>[0.0037477333, -0.030682955, -0.043211866, -0....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>Why Did God Create Us?</td>\n",
       "      <td>YOR</td>\n",
       "      <td>Does God need to be glorified?</td>\n",
       "      <td>If worship is a response to God, then did God ...</td>\n",
       "      <td>[-0.049316168, -0.10144768, -0.025615225, -0.0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>Why Did God Create Us?</td>\n",
       "      <td>AUD</td>\n",
       "      <td>Does God need to be glorified?</td>\n",
       "      <td>God doesn't require praise</td>\n",
       "      <td>[-0.049430676, -0.089813165, -0.101273105, -0....</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Why Did God Create Us?</td>\n",
       "      <td>TMS</td>\n",
       "      <td>If there is a creator, How would the creator b...</td>\n",
       "      <td>All the good things that we know are of His cr...</td>\n",
       "      <td>[0.012368551, -0.025174651, 0.024788452, -0.06...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Why Did God Create Us?</td>\n",
       "      <td>GRE</td>\n",
       "      <td>If there is a creator, How would the creator b...</td>\n",
       "      <td>The creator would be a dictator of value then,...</td>\n",
       "      <td>[-0.08305474, 0.071656995, -0.00630112, 0.0222...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>Why Did God Create Us?</td>\n",
       "      <td>AMD</td>\n",
       "      <td>If there is a creator, How would the creator b...</td>\n",
       "      <td>Everything that is outside of God would be wrong</td>\n",
       "      <td>[0.018025791, 0.011236955, -0.09136005, -0.018...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>Why Did God Create Us?</td>\n",
       "      <td>GRE</td>\n",
       "      <td>If there is a creator, How would the creator b...</td>\n",
       "      <td>If a person does not accept the creator as tr...</td>\n",
       "      <td>[-0.039067924, 0.02558411, -0.088014774, -0.06...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Why Did God Create Us?</td>\n",
       "      <td>TMS</td>\n",
       "      <td>Why would a Holy God create a creation that ha...</td>\n",
       "      <td>Unholiness is the output of free will</td>\n",
       "      <td>[-0.20886505, 0.21488056, -0.1136066, -0.03556...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>Why Did God Create Us?</td>\n",
       "      <td>ANT</td>\n",
       "      <td>Why would a Holy God create a creation that ha...</td>\n",
       "      <td>Maybe because there would be balance in a cert...</td>\n",
       "      <td>[-0.03254736, 0.038270056, -0.10513653, -0.058...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>Why Did God Create Us?</td>\n",
       "      <td>YOR</td>\n",
       "      <td>Why would a Holy God create a creation that ha...</td>\n",
       "      <td>Creating something possessing a freewill inna...</td>\n",
       "      <td>[-0.07560672, 0.0037901227, -0.061460886, -0.0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>Why Did God Create Us?</td>\n",
       "      <td>GRE</td>\n",
       "      <td>Why would a Holy God create a creation that ha...</td>\n",
       "      <td>This concept is too Christian</td>\n",
       "      <td>[-0.05881997, -0.08591559, -0.19163653, 0.0403...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>Why Did God Create Us?</td>\n",
       "      <td>SAM</td>\n",
       "      <td>Why would a Holy God create a creation that ha...</td>\n",
       "      <td>Why is love not objective then?</td>\n",
       "      <td>[-0.11373837, 0.061778314, 0.03615011, -0.1461...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>Why Did God Create Us?</td>\n",
       "      <td>GRE</td>\n",
       "      <td>Why would a Holy God create a creation that ha...</td>\n",
       "      <td>Because not everbody understands the concept o...</td>\n",
       "      <td>[-0.10172667, -0.056891102, -0.008180991, 0.04...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                event_title speaker  \\\n",
       "id                                    \n",
       "176  Why Did God Create Us?      PY   \n",
       "177  Why Did God Create Us?     GRE   \n",
       "178  Why Did God Create Us?      PY   \n",
       "179  Why Did God Create Us?      JJ   \n",
       "180  Why Did God Create Us?     TMS   \n",
       "181  Why Did God Create Us?     TMS   \n",
       "182  Why Did God Create Us?      DJ   \n",
       "183  Why Did God Create Us?     RIC   \n",
       "184  Why Did God Create Us?     GRE   \n",
       "185  Why Did God Create Us?     TMS   \n",
       "186  Why Did God Create Us?     RIC   \n",
       "187  Why Did God Create Us?     PAU   \n",
       "188  Why Did God Create Us?     PAU   \n",
       "189  Why Did God Create Us?     LIZ   \n",
       "190  Why Did God Create Us?     RIC   \n",
       "191  Why Did God Create Us?     AMD   \n",
       "192  Why Did God Create Us?     TMS   \n",
       "193  Why Did God Create Us?     YOR   \n",
       "194  Why Did God Create Us?     AUD   \n",
       "195  Why Did God Create Us?     TMS   \n",
       "196  Why Did God Create Us?     GRE   \n",
       "197  Why Did God Create Us?     AMD   \n",
       "198  Why Did God Create Us?     GRE   \n",
       "199  Why Did God Create Us?     TMS   \n",
       "200  Why Did God Create Us?     ANT   \n",
       "201  Why Did God Create Us?     YOR   \n",
       "202  Why Did God Create Us?     GRE   \n",
       "203  Why Did God Create Us?     SAM   \n",
       "204  Why Did God Create Us?     GRE   \n",
       "\n",
       "                                              question  \\\n",
       "id                                                       \n",
       "176                          How did we come to exist?   \n",
       "177                          How did we come to exist?   \n",
       "178                    Do you think you need to exist?   \n",
       "179                    Do you think you need to exist?   \n",
       "180                    Do you think you need to exist?   \n",
       "181                    Do you think you need to exist?   \n",
       "182                    Do you think you need to exist?   \n",
       "183                    Do you think you need to exist?   \n",
       "184  From a bigger perspective, why did God create us?   \n",
       "185  From a bigger perspective, why did God create us?   \n",
       "186  From a bigger perspective, why did God create us?   \n",
       "187  From a bigger perspective, why did God create us?   \n",
       "188  From a bigger perspective, why did God create us?   \n",
       "189  From a bigger perspective, why did God create us?   \n",
       "190  From a bigger perspective, why did God create us?   \n",
       "191                     Does God need to be glorified?   \n",
       "192                     Does God need to be glorified?   \n",
       "193                     Does God need to be glorified?   \n",
       "194                     Does God need to be glorified?   \n",
       "195  If there is a creator, How would the creator b...   \n",
       "196  If there is a creator, How would the creator b...   \n",
       "197  If there is a creator, How would the creator b...   \n",
       "198  If there is a creator, How would the creator b...   \n",
       "199  Why would a Holy God create a creation that ha...   \n",
       "200  Why would a Holy God create a creation that ha...   \n",
       "201  Why would a Holy God create a creation that ha...   \n",
       "202  Why would a Holy God create a creation that ha...   \n",
       "203  Why would a Holy God create a creation that ha...   \n",
       "204  Why would a Holy God create a creation that ha...   \n",
       "\n",
       "                                                answer  \\\n",
       "id                                                       \n",
       "176  Everybody should believe that they are creatio...   \n",
       "177   This is undeniable, and the Charles Darwin's ...   \n",
       "178                      We don't really need to exist   \n",
       "179                     I don't think we need to exist   \n",
       "180          I think we are a part of a bigger picture   \n",
       "181  Yes, because we're a part of a community where...   \n",
       "182         In general,  humans have made many impacts   \n",
       "183  This got me thinking, if we question the purpo...   \n",
       "184                                     It's a mystery   \n",
       "185  I think God himself needs an outlet to His lov...   \n",
       "186                      God created us to glorify Him   \n",
       "187                           God is full of mysteries   \n",
       "188  I too believe that humans are created for a pu...   \n",
       "189  Maybe because God is a relational being, so He...   \n",
       "190  I reckon God created humans so the world would...   \n",
       "191  if we are indeed created to glorify Him, does ...   \n",
       "192  i think if that's the only reason for humans t...   \n",
       "193  If worship is a response to God, then did God ...   \n",
       "194                         God doesn't require praise   \n",
       "195  All the good things that we know are of His cr...   \n",
       "196  The creator would be a dictator of value then,...   \n",
       "197   Everything that is outside of God would be wrong   \n",
       "198   If a person does not accept the creator as tr...   \n",
       "199              Unholiness is the output of free will   \n",
       "200  Maybe because there would be balance in a cert...   \n",
       "201   Creating something possessing a freewill inna...   \n",
       "202                      This concept is too Christian   \n",
       "203                    Why is love not objective then?   \n",
       "204  Because not everbody understands the concept o...   \n",
       "\n",
       "                                        Document Embed  Cluster Assignment  \n",
       "id                                                                          \n",
       "176  [0.006212553, 0.038233534, -0.024160838, -0.10...                   0  \n",
       "177  [-0.007556731, -0.009657541, 0.0465072, -0.032...                   0  \n",
       "178  [-0.11323776, -0.015670797, -0.079038456, 0.09...                   0  \n",
       "179  [-0.1383083, -0.005404059, -0.09285827, 0.1340...                   0  \n",
       "180  [-0.0036777146, 0.0565521, -0.049426157, -0.08...                  -1  \n",
       "181  [-0.0068717278, -0.053223114, -0.028907582, -0...                   0  \n",
       "182  [-0.0020462424, 0.08268591, -0.013377609, -0.1...                   0  \n",
       "183  [-0.061571285, -0.009830134, -0.029011544, -0....                   0  \n",
       "184  [-0.17551, -0.082361, 0.35437, 0.057444, 0.627...                  -1  \n",
       "185  [-0.05835578, -0.04148901, -0.0784467, -0.0107...                   0  \n",
       "186  [0.022100516, 0.03634443, -0.17097521, -0.0331...                   1  \n",
       "187  [-0.120145984, -0.08685847, 0.027744427, -0.06...                  -1  \n",
       "188  [-0.07333655, 0.079368055, -0.15811524, -0.111...                  -1  \n",
       "189  [-0.16157854, -0.012918956, -0.015164363, -0.0...                   0  \n",
       "190  [-0.004739574, 0.11398921, -0.101472095, -0.05...                   0  \n",
       "191  [0.042036586, 0.09319516, -0.1104986, 0.015954...                   1  \n",
       "192  [0.0037477333, -0.030682955, -0.043211866, -0....                   0  \n",
       "193  [-0.049316168, -0.10144768, -0.025615225, -0.0...                   0  \n",
       "194  [-0.049430676, -0.089813165, -0.101273105, -0....                  -1  \n",
       "195  [0.012368551, -0.025174651, 0.024788452, -0.06...                   0  \n",
       "196  [-0.08305474, 0.071656995, -0.00630112, 0.0222...                  -1  \n",
       "197  [0.018025791, 0.011236955, -0.09136005, -0.018...                   0  \n",
       "198  [-0.039067924, 0.02558411, -0.088014774, -0.06...                   0  \n",
       "199  [-0.20886505, 0.21488056, -0.1136066, -0.03556...                  -1  \n",
       "200  [-0.03254736, 0.038270056, -0.10513653, -0.058...                   0  \n",
       "201  [-0.07560672, 0.0037901227, -0.061460886, -0.0...                   0  \n",
       "202  [-0.05881997, -0.08591559, -0.19163653, 0.0403...                  -1  \n",
       "203  [-0.11373837, 0.061778314, 0.03615011, -0.1461...                  -1  \n",
       "204  [-0.10172667, -0.056891102, -0.008180991, 0.04...                   0  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTotal = myGoods.append(myOutliers)\n",
    "\n",
    "dfTotalByID = dfTotal.groupby(\"id\")\n",
    "\n",
    "# dfTotalByID.get_group('176')\n",
    "dfTotalByID.first()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "efebef93848af86820ba5c9af15e3b0ea109bf901e8b1e27dbaca7b722da8278"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
