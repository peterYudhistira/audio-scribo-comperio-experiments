{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports go here\n",
    "import numpy as np\n",
    "import db\n",
    "import pandas as pd\n",
    "import inflect\n",
    "import string\n",
    "import nltk\n",
    "import gensim\n",
    "import contractions\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim.downloader as api\n",
    "from gensim import corpora, models\n",
    "from nltk.test.gensim_fixt import setup_module\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# this is where everything we've experimented on will be implemented.\n",
    "\n",
    "\n",
    "class AnomalyDetector():\n",
    "    def __init__(self, dbName: str = \"\",  dh=None, model=None, modelName=\"glove-wiki-gigaword-300\") -> None:\n",
    "        if dh is None:\n",
    "            self.dh = db.DatabaseHandler(dbName=dbName)\n",
    "        else:\n",
    "            self.dh = dh\n",
    "        if model is None:\n",
    "            if modelName != \"\":\n",
    "                self.model = api.load(modelName)\n",
    "        else:\n",
    "            self.model = model\n",
    "\n",
    "        self.FeatureExtractionParams = {}\n",
    "        self.AnomalyDetectionParams = {}\n",
    "\n",
    "    '''\n",
    "    inputs :\n",
    "    - dh : DatabaseHandler --> to retrieve data from database\n",
    "    - eventID : int --> we're doing this by event, so straight to the eventID\n",
    "    - selector : str --> pretty much formality.\n",
    "    - splitBySentences : bool --> Split each doc into sentences or not. Defaults to no.\n",
    "    '''\n",
    "    '''\n",
    "    outputs:\n",
    "    None, just setting\n",
    "    '''\n",
    "\n",
    "    def SetDFFromDB(self, eventID: int, selector: str = \"event_id\", splitBySentences: bool = False):\n",
    "        self.df = self.dh.get_recordDataJoinedDF(selector=selector, ID=eventID)\n",
    "        if splitBySentences:\n",
    "            # df.set_index('id', inplace=True)\n",
    "            self.df['answer'] = self.df['answer'].str.split('.')\n",
    "            self.df = self.df.explode(\"answer\", True)\n",
    "            self.df.drop(self.df[self.df[\"answer\"] == \"\"].index, inplace=True)\n",
    "            self.df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # ditto above, but takes a pre-made DF instead.\n",
    "    def SetDF(self, df: db.pd.DataFrame, splitBySentences: bool = False):\n",
    "        self.df = df\n",
    "        if splitBySentences:\n",
    "            # df.set_index('id', inplace=True)\n",
    "            self.df['answer'] = self.df['answer'].str.split('.')\n",
    "            self.df = self.df.explode(\"answer\", True)\n",
    "            self.df.drop(self.df[self.df[\"answer\"] == \"\"].index, inplace=True)\n",
    "            self.df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    def SetModel(self, modelName: str = \"glove-wiki-gigaword-300\"):\n",
    "        self.model = api.load(modelName)\n",
    "\n",
    "    # these are to add key:value to the dictionaries that dictate parameters. Indeed, we are refurbishing.\n",
    "    def SetFeatureExtractionParam(self, key: str, value):\n",
    "        self.FeatureExtractionParams[key] = value\n",
    "\n",
    "    def SetAnomalyDetectionParam(self, key: str, value):\n",
    "        self.AnomalyDetectionParams[key] = value\n",
    "\n",
    "    '''\n",
    "    inputs :\n",
    "    - dh : DatabaseHandler --> to retrieve data from database\n",
    "    - eventID : int --> we're doing this by event, so straight to the eventID\n",
    "    - selector : str --> pretty much formality.\n",
    "    - splitBySentences : bool --> Split each doc into sentences or not. Defaults to no.\n",
    "    '''\n",
    "    '''\n",
    "    outputs:\n",
    "    - df : DataFrame --> dataframe containing the thing we're gonna be using.\n",
    "    '''\n",
    "\n",
    "    def GetDF(self, dh: db.DatabaseHandler, eventID: int, selector: str = \"event_id\", splitBySentences: bool = False):\n",
    "        df = dh.get_recordDataJoinedDF(selector=selector, ID=eventID)\n",
    "        if splitBySentences:\n",
    "            # df.set_index('id', inplace=True)\n",
    "            df['answer'] = df['answer'].str.split('.')\n",
    "            df = df.explode(\"answer\", True)\n",
    "            df.drop(df[df[\"answer\"] == \"\"].index, inplace=True)\n",
    "            df.reset_index(drop=True, inplace=True)\n",
    "        return df\n",
    "\n",
    "    '''\n",
    "    inputs:\n",
    "    - doc : str --> a string representing a sentence/document.\n",
    "    - isLemma : bool --> use lemmatizer or not? Defaults to not.\n",
    "    - isStopWords : bool --> use stopwords or not? Defaults to not.\n",
    "    - isInflect : bool --> use inflections (you're --> you are) or not? Defaults to not.\n",
    "    - isNumberFiltered :  bool --> delete numbers in the string? Defaults to yes. \n",
    "    '''\n",
    "    '''\n",
    "    output : list<str> --> a list of word tokens (list<string>)\n",
    "    '''\n",
    "\n",
    "    def PreprocessDocument(self, doc: str, isLemma: bool = False, isStopWords: bool = False, isInflect: bool = False, isNumberFiltered: bool = True):\n",
    "        inflector = inflect.engine()\n",
    "        stopwordSet = set(stopwords.words(\"english\"))\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        punctuations = string.punctuation\n",
    "        # if numbers are filtered, add that to the punctuation string\n",
    "        if isNumberFiltered:\n",
    "            punctuations += \"1234567890\"\n",
    "\n",
    "        # case fold\n",
    "        doc = doc.lower()\n",
    "\n",
    "        # remove puncs\n",
    "        doc = \"\".join([char for char in doc if char not in punctuations])\n",
    "\n",
    "        # tokenize it.\n",
    "        token_list = nltk.word_tokenize(doc)\n",
    "\n",
    "        for i in range(len(token_list)):\n",
    "            # if inflect\n",
    "            if isInflect:\n",
    "                if token_list[i].isdigit():\n",
    "                    token_list[i] = inflector.number_to_words(token_list[i])\n",
    "\n",
    "            # if lemma\n",
    "            if isLemma:\n",
    "                tagged_word = nltk.pos_tag([token_list[i]])\n",
    "                wordnet_pos = self.getWordnetPos(tagged_word[0][1])\n",
    "                token_list[i] = lemmatizer.lemmatize(\n",
    "                    tagged_word[0][0], pos=wordnet_pos)\n",
    "\n",
    "            # if stopword\n",
    "            if isStopWords:\n",
    "                if token_list[i] in stopwordSet or token_list[i].isdigit():\n",
    "                    token_list[i] = \"#\"  # mark as #\n",
    "\n",
    "        # remove the marked strings\n",
    "        token_list = [token for token in token_list if token != \"#\"]\n",
    "\n",
    "        if token_list:\n",
    "            return token_list\n",
    "        return [\"\"]\n",
    "\n",
    "    '''\n",
    "    inputs:\n",
    "    - tag : str --> the tag obtained from POS tagging.\n",
    "    '''\n",
    "    '''\n",
    "    outputs:\n",
    "    - str --> Wordnet POS tag.\n",
    "    '''\n",
    "\n",
    "    def getWordnetPos(self, tag):\n",
    "        \"\"\"Map POS tag to WordNet POS tag\"\"\"\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN  # solves as noun by default.\n",
    "\n",
    "    '''\n",
    "    inputs:\n",
    "    - doclist : list<str> --> list of doc/sentences.\n",
    "    - isProcessed : bool --> has it already been preprocessed? Defaults to True.\n",
    "    '''\n",
    "    '''\n",
    "    outputs:\n",
    "    - df_tfidf : Dataframe --> the TFIDF matrix in df form. \n",
    "    - matrix : matrix --> the TFIDF matrix purely. mainly for LDA purposes.\n",
    "    '''\n",
    "\n",
    "    def GetTFIDF(self, doclist: list, isPreprocessed=True):\n",
    "        if not isPreprocessed:\n",
    "            doclist = [self.PreprocessDocument(\n",
    "                doc, isLemma=True, isStopWords=True) for doc in doclist]\n",
    "        # else:\n",
    "        #     # just tokenize the thing\n",
    "        #     doclist = [nltk.word_tokenize(doc) for doc in doclist]\n",
    "        # i think the thing has already been tokenized. That's the problem.\n",
    "        flat_doclist = [' '.join(doc)\n",
    "                        for doc in doclist]  # turn into one big corpus\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        matrix = vectorizer.fit_transform(flat_doclist)\n",
    "        tfidf_keys = vectorizer.get_feature_names_out()\n",
    "        df_tfidf = db.pd.DataFrame(matrix.toarray(), columns=tfidf_keys)\n",
    "\n",
    "        return df_tfidf, matrix\n",
    "\n",
    "    # input : list<str> : tokens of one document/sentence\n",
    "    # output : list<(str, list<int>[300])> : list of word-vector pair for each word available on the model\n",
    "    def WordEmbed(self, document: list, model):\n",
    "        word_embed_pairs = []\n",
    "        for word in document:\n",
    "            if word in model:\n",
    "                word_embed_pairs.append((word, model[word]))\n",
    "        return word_embed_pairs\n",
    "\n",
    "    '''\n",
    "    inputs :\n",
    "    - model         :           -->word2vec model\n",
    "    - document      : list<str> --> a list of word tokens to embed.\n",
    "    - maxLength     : int       --> the maximum length, to pad the vector with if necessary.\n",
    "    '''\n",
    "    def TinyWordEmbed(self, document:list, model, maxLength:int):\n",
    "        features = []\n",
    "        # this creates a feature length of len(document)\n",
    "        for word in document:\n",
    "            if word in model:\n",
    "                features.append(np.mean(model[word], axis=0))\n",
    "\n",
    "        # if less than maxLength, pad with zeros.\n",
    "        if len(features) < maxLength:\n",
    "            padLength = maxLength - len(features)\n",
    "            padding = np.zeros(padLength)\n",
    "            features = np.concatenate((features, padding))\n",
    "            \n",
    "        # i remain pessimistic that this would work.\n",
    "        return features\n",
    "\n",
    "\n",
    "    # input : list<(str, list<float>[300])>, str : word-vector pair list and preferred agg method.\n",
    "    # output : list<float>[300] : 300-d vector that represents an aggregated value of the input words\n",
    "    def SentenceEmbedUnweightedFunction(self, word_embed_pair_list: list, aggregateMethod: str = \"avg\"):\n",
    "        wvs = []\n",
    "        for pair in word_embed_pair_list:\n",
    "            wvs.append(pair[1])\n",
    "        if aggregateMethod == \"avg\":\n",
    "            return np.mean(wvs, axis=0)\n",
    "        else:\n",
    "            return np.sum(wvs, axis=0)\n",
    "\n",
    "    # input : list<list<(str, list<float>[300])>>, str : list containing word-vector pairs and preferred agg method\n",
    "    # output : list<(str, list<int>[300])> : list containing sentence-vector pairs.\n",
    "    def SentenceEmbedUnweighted(self, word_embedded_docs: list, aggregateMethod: str = \"avg\"):\n",
    "        sentence_embedded_docs = []\n",
    "        for i in range(len(word_embedded_docs)):\n",
    "            sentence_embedded_docs.append(self.SentenceEmbedUnweightedFunction(\n",
    "                word_embedded_docs[i], aggregateMethod))\n",
    "        return sentence_embedded_docs\n",
    "\n",
    "    '''\n",
    "    input :\n",
    "    list<list<(str, list<float>[300])>> : word-vector pair list\n",
    "    matrix : tf-idf matrix for the corresponding doc\n",
    "    int : the row we want\n",
    "    str : preferred agg method\n",
    "    '''\n",
    "    # output : list<float>[300] : 300-d vector that represents an aggregated value of the input words\n",
    "\n",
    "    def SentenceEmbedWeightedFunction(self, word_embed_pair_list: list, tfidf_matrix, index: int, aggregateMethod: str = \"avg\"):\n",
    "        weighted_wvs = []\n",
    "        # multiplies each word with its TF-IDF value in the corresponding row. Is 0 if word isn't found somehow.\n",
    "        for pair in word_embed_pair_list:\n",
    "            tfidf_weight = 0\n",
    "            if pair[0] in tfidf_matrix:\n",
    "                tfidf_weight = tfidf_matrix[pair[0]][index]\n",
    "            weighted_wvs.append(pair[1] * tfidf_weight)\n",
    "        # turn into array for fast aggregating\n",
    "        weighted_wvs = np.array(weighted_wvs)\n",
    "        if aggregateMethod == \"avg\":\n",
    "            sentence_vector = np.mean(weighted_wvs, axis=0)\n",
    "        else:\n",
    "            sentence_vector = np.sum(weighted_wvs, axis=0)\n",
    "        return sentence_vector\n",
    "\n",
    "    # input : list<list<(str, list<float>[300])>>, str : list containing word-vector pairs, TF-IDF matrix of the corpus, and preferred agg method\n",
    "    # output : list<(str, list<float>[300])> : list containing sentence-vector pairs.\n",
    "    def SentenceEmbedWeighted(self, word_embedded_docs: list, tfidf_matrix, aggregateMethod=\"avg\"):\n",
    "        sentence_embedded_docs = []\n",
    "        for i in range(len(word_embedded_docs)):\n",
    "            sentence_embedded_docs.append(self.SentenceEmbedWeightedFunction(\n",
    "                word_embedded_docs[i], tfidf_matrix, i, aggregateMethod))\n",
    "        return sentence_embedded_docs\n",
    "\n",
    "    '''\n",
    "    input:\n",
    "    - doclist : list<list<str>> --> list of tokenized sentences/docs\n",
    "    - topics : int --> number of inferred topics.\n",
    "    - use_tfidf : bool --> use TFIDF or not? defaults to yes.\n",
    "    '''\n",
    "    '''\n",
    "    output:\n",
    "    - docFeatureList : list<list<float>> --> topic distribution for each sentence/doc\n",
    "    '''\n",
    "\n",
    "    def GetLDADistribution(self, doclist: list, topics: int = 5, use_tfidf: bool = True):\n",
    "        new_corpus = []\n",
    "\n",
    "        if use_tfidf:\n",
    "            for i in range(len(doclist)):\n",
    "                doc = [(j, self.tfidf_matrix[i, j])\n",
    "                       for j in self.tfidf_matrix[i].indices]\n",
    "                new_corpus.append(doc)\n",
    "                gensim_dict = corpora.Dictionary.from_corpus(new_corpus)\n",
    "        else:\n",
    "            gensim_dict = corpora.Dictionary(doclist)\n",
    "            new_corpus = [gensim_dict.doc2bow(doc) for doc in doclist]\n",
    "\n",
    "        lda_model = gensim.models.LdaModel(\n",
    "            new_corpus, num_topics=topics, id2word=gensim_dict)\n",
    "        goofy_ahh_doc_topic_distributions = lda_model[new_corpus]\n",
    "\n",
    "        docFeatureList = []\n",
    "        for doc_topic_dist in goofy_ahh_doc_topic_distributions:\n",
    "            featureList = [0.0 for i in range(0, topics)]\n",
    "            for topic_dist in doc_topic_dist:\n",
    "                featureList[topic_dist[0]] = topic_dist[1]\n",
    "            docFeatureList.append(featureList)\n",
    "\n",
    "        return docFeatureList\n",
    "\n",
    "    '''\n",
    "    inputs:\n",
    "    - vectors : list<list<float>> --> list of features corresponding to each doc/sentence\n",
    "    - epsilon : float --> the radius within which points are considered connected.\n",
    "    - min : int --> minimum amount of connected points for a point to be considered a core point of a cluster.\n",
    "    '''\n",
    "    '''\n",
    "    output:\n",
    "    clusters : list<int> --> a list of integers to assign each data point to a cluster. -1 means outlier.\n",
    "    '''\n",
    "\n",
    "    def GetDBSCANClusters(self, vectors, epsilon: float, min: int):\n",
    "        dbscan = DBSCAN(eps=epsilon, min_samples=min)\n",
    "        clusters = dbscan.fit_predict(vectors)\n",
    "        # plt.title(\"to the depths of depravity {} and the cusp of blasphemy {}.\".format(epsilon, min))\n",
    "        # plt.scatter(vectors[:, 0], vectors[:, 1], c=clusters)\n",
    "        # plt.show()\n",
    "        # print(clusters)\n",
    "        return clusters\n",
    "    '''\n",
    "    inputs:\n",
    "    - vectors : list<list<float>> --> list of features corresponding to each doc/sentence\n",
    "\n",
    "    '''\n",
    "\n",
    "    def GetIFResults(self, vector):\n",
    "        isolationForest = IsolationForest(n_estimators=500, contamination=0.1)\n",
    "        isolationForest.fit(vector)\n",
    "        IFResults = isolationForest.decision_function(vector)\n",
    "\n",
    "        # minus values yield anomalies.\n",
    "        for i in range(len(IFResults)):\n",
    "            if IFResults[i] >= 0:\n",
    "                IFResults[i] = 0\n",
    "            else:\n",
    "                IFResults[i] = -1\n",
    "        return IFResults\n",
    "\n",
    "    '''\n",
    "    inputs :\n",
    "    - clusters : list<int> --> a list of clusters assigned to each doc/sentence\n",
    "    - df : DataFrame --> the dataframe in question\n",
    "    - isReturnSeparate : bool --> split return or not. Defaults to split (for some reason...)\n",
    "    '''\n",
    "    '''\n",
    "    outputs:\n",
    "    - dfOutliers : DataFrame --> the dataframe whose answers have been marked as outliers.\n",
    "    - dfGoods : DataFrame --> the dataframe whose answers have not been marked as outliers.\n",
    "    '''\n",
    "\n",
    "    def ReturnClusters(self, isReturnSeparate: bool = True):\n",
    "        if isReturnSeparate:\n",
    "            dfGoods = self.df.loc[self.df[\"Cluster Assignment\"] != -1]\n",
    "            dfGoods.reset_index(inplace=True)\n",
    "            dfOutliers = self.df.loc[self.df[\"Cluster Assignment\"] == -1]\n",
    "            dfOutliers.reset_index(inplace=True)\n",
    "            return dfOutliers, dfGoods\n",
    "        else:\n",
    "            if self.df.isnull().values.any():\n",
    "                self.df.reset_index(inplace=True)\n",
    "            return self.df\n",
    "\n",
    "    '''\n",
    "    inputs:\n",
    "    - method : str --> LDA or Embedding.\n",
    "    - isWeighted : bool --> use weights or not\n",
    "    - nTopics : int --> for LDA.\n",
    "    '''\n",
    "    '''\n",
    "    - outputs : none. This is an internal function\n",
    "    '''\n",
    "    def GetAnomalies(self, isReturnSeparate: bool = False):\n",
    "        self.SetDocumentTokens()  # set tokens in the DF\n",
    "        if self.FeatureExtractionParams[\"method\"] == \"Embedding\":\n",
    "            self.SetEmbeddingResult()\n",
    "        elif self.FeatureExtractionParams[\"method\"] == \"LDA\":\n",
    "            self.SetLDAResult()\n",
    "        elif self.FeatureExtractionParams[\"method\"] == \"Tiny\":\n",
    "            self.SetTinyEmbeddingResult()\n",
    "\n",
    "        if self.AnomalyDetectionParams[\"algorithm\"] == \"DBSCAN\":\n",
    "            self.SetDBSCANClusters(list(self.df[\"Document Embed\"]))\n",
    "        elif self.AnomalyDetectionParams[\"algorithm\"] == \"LOF\":\n",
    "            self.SetLOFClusters(list(self.df[\"Document Embed\"]))\n",
    "        elif self.AnomalyDetectionParams[\"algorithm\"] == \"IF\":\n",
    "            self.SetIFClusters(list(self.df[\"Document Embed\"]))\n",
    "\n",
    "        return self.ReturnClusters(isReturnSeparate=isReturnSeparate)\n",
    "\n",
    "    '''\n",
    "    inputs  : None (checks self.FeatureExtractionParams)\n",
    "    desc    : embeds each doc and put it in a new column \"Document Embed\"\n",
    "    '''\n",
    "\n",
    "    def SetEmbeddingResult(self):\n",
    "        # extract feature with embedding\n",
    "        self.wordEmbeddedDocs = [self.WordEmbed(\n",
    "            doc, self.model) for doc in self.preprocessedDocs]\n",
    "\n",
    "        if \"weighted\" in self.FeatureExtractionParams:\n",
    "            self.tfidf_df, self.tfidf_matrix = self.GetTFIDF(\n",
    "                self.preprocessedDocs)\n",
    "            self.doc_embeds = self.SentenceEmbedWeighted(\n",
    "                self.wordEmbeddedDocs, self.tfidf_df, self.FeatureExtractionParams[\"aggregate_method\"])\n",
    "        else:\n",
    "            self.doc_embeds = self.SentenceEmbedUnweighted(\n",
    "                self.wordEmbeddedDocs, self.FeatureExtractionParams[\"aggregate_method\"])\n",
    "\n",
    "        self.df[\"Document Embed\"] = self.doc_embeds\n",
    "\n",
    "    def SetTinyEmbeddingResult(self):\n",
    "        # get max length\n",
    "        maxdoc = max(self.preprocessedDocs, key=len )\n",
    "        maxlen = len(maxdoc)\n",
    "        # print(\"maxdoc : {}, maxlen : {}\".format(maxdoc, maxlen))\n",
    "        # extract feature of each word with embedding\n",
    "        self.doc_embeds = [self.TinyWordEmbed(\n",
    "            doc, self.model, maxlen) for doc in self.preprocessedDocs]\n",
    "        \n",
    "        self.df[\"Document Embed\"] =  self.doc_embeds\n",
    "        \n",
    "    def SetDefaultParams(self):\n",
    "        # here we will put the default params\n",
    "        self.SetFeatureExtractionParam(\"method\", \"Embedding\")\n",
    "        self.SetFeatureExtractionParam(\"weighted\", True)\n",
    "        self.SetFeatureExtractionParam(\"condense\", False)\n",
    "        self.SetFeatureExtractionParam(\"n_topics\", 5)\n",
    "        self.SetFeatureExtractionParam(\"aggregate_method\", \"avg\")\n",
    "        self.SetAnomalyDetectionParam(\"algorithm\", \"DBSCAN\")\n",
    "        self.SetAnomalyDetectionParam(\"epsilon\", 1.0)\n",
    "        self.SetAnomalyDetectionParam(\"minsamp\", 2)\n",
    "        self.SetAnomalyDetectionParam(\"epsilon\", 1.0)\n",
    "        self.SetAnomalyDetectionParam(\"algorithm\", \"IF\")\n",
    "        self.SetAnomalyDetectionParam(\"estimators\", 500)\n",
    "        self.SetAnomalyDetectionParam(\"contamination\", 0.1)\n",
    "        self.SetAnomalyDetectionParam(\"neighbors\", 5)\n",
    "\n",
    "    # preprocess each doc/sentence\n",
    "    def SetDocumentTokens(self):\n",
    "        self.preprocessedDocs = [self.PreprocessDocument(\n",
    "            doc, isLemma=True, isStopWords=True) for doc in self.df[\"answer\"]]\n",
    "        self.df[\"Tokenized\"] = self.preprocessedDocs\n",
    "\n",
    "        # if cut off data with less than x values\n",
    "        if \"prune\" in self.FeatureExtractionParams:\n",
    "            mask = self.df['Embedded Docs'].apply(\n",
    "                lambda x: len(x) > self.FeatureExtractionParams[\"prune\"])\n",
    "            self.df = self.df[mask]\n",
    "\n",
    "    '''\n",
    "    inputs  : None (checks self.FeatureExtractionParams)\n",
    "    desc    : assigns topic distribution for each document.\n",
    "    '''\n",
    "\n",
    "    def SetLDAResult(self):\n",
    "        doclist = list(self.df[\"Tokenized\"])\n",
    "        new_corpus = []\n",
    "        if self.FeatureExtractionParams[\"weighted\"]:\n",
    "            self.tfidf_df, self.tfidf_matrix = self.GetTFIDF(\n",
    "                self.preprocessedDocs)\n",
    "            for i in range(len(doclist)):\n",
    "                doc = [(j, self.tfidf_matrix[i, j])\n",
    "                       for j in self.tfidf_matrix[i].indices]\n",
    "                new_corpus.append(doc)\n",
    "                gensim_dict = corpora.Dictionary.from_corpus(new_corpus)\n",
    "        else:\n",
    "            gensim_dict = corpora.Dictionary(doclist)\n",
    "            new_corpus = [gensim_dict.doc2bow(doc) for doc in doclist]\n",
    "\n",
    "        lda_model = gensim.models.LdaModel(\n",
    "            new_corpus, num_topics=self.FeatureExtractionParams[\"n_topics\"], id2word=gensim_dict)\n",
    "        goofy_ahh_doc_topic_distributions = lda_model[new_corpus]\n",
    "\n",
    "        docFeatureList = []\n",
    "        for doc_topic_dist in goofy_ahh_doc_topic_distributions:\n",
    "            featureList = [0.0 for i in range(\n",
    "                0, self.FeatureExtractionParams[\"n_topics\"])]\n",
    "            for topic_dist in doc_topic_dist:\n",
    "                featureList[topic_dist[0]] = topic_dist[1]\n",
    "            docFeatureList.append(featureList)\n",
    "\n",
    "        self.df[\"Document Embed\"] = docFeatureList\n",
    "\n",
    "    '''\n",
    "    inputs  :\n",
    "    - vectors : list<list<float>> --> list of features corresponding to each doc/sentence\n",
    "    desc    : assigns cluster via DBSCAN. \n",
    "    '''\n",
    "\n",
    "    def SetDBSCANClusters(self, vectors):\n",
    "        dbscan = DBSCAN(\n",
    "            eps=self.AnomalyDetectionParams[\"epsilon\"], min_samples=self.AnomalyDetectionParams[\"minsamp\"])\n",
    "        clusters = dbscan.fit_predict(vectors)\n",
    "        self.df[\"Cluster Assignment\"] = clusters\n",
    "\n",
    "    '''\n",
    "    inputs:\n",
    "    - vectors : list<list<float>> --> list of features corresponding to each doc/sentence\n",
    "\n",
    "    '''\n",
    "\n",
    "    def SetIFClusters(self, vector):\n",
    "        isolationForest = IsolationForest(\n",
    "            n_estimators=self.AnomalyDetectionParams[\"estimators\"], contamination=self.AnomalyDetectionParams[\"contamination\"])\n",
    "        isolationForest.fit(vector)\n",
    "        IFResults = isolationForest.decision_function(vector)\n",
    "\n",
    "        # minus values yield anomalies.\n",
    "        for i in range(len(IFResults)):\n",
    "            if IFResults[i] >= 0:\n",
    "                IFResults[i] = 0\n",
    "            else:\n",
    "                IFResults[i] = -1\n",
    "        self.df[\"Cluster Assignment\"] = IFResults\n",
    "\n",
    "    '''\n",
    "    inputs : vectors : list<list<float>> --> list of features for each doc/sentence\n",
    "    '''\n",
    "\n",
    "    def SetLOFClusters(self, vector):\n",
    "        lof = LocalOutlierFactor(\n",
    "            n_neighbors=self.AnomalyDetectionParams[\"neighbors\"], contamination=self.AnomalyDetectionParams[\"contamination\"])\n",
    "        lof.fit(vector)\n",
    "        LOFResults = lof.negative_outlier_factor_\n",
    "\n",
    "        # minus values yield anomalies\n",
    "        if \"lof_generalize\" in self.AnomalyDetectionParams and self.AnomalyDetectionParams[\"lof_generalize\"]:\n",
    "            print(\"if minus we go bald\")\n",
    "            LOFResults[LOFResults >= 0] = 0\n",
    "            LOFResults[LOFResults < 0] = -1\n",
    "        else:\n",
    "            # if not generalize, we assume outliers are within the n-th percentile, with n = contamination rate.\n",
    "            print(\"taking the {}-th percentile\".format(self.AnomalyDetectionParams[\"contamination\"] * 100))\n",
    "            threshold = np.percentile(LOFResults, 100 * self.AnomalyDetectionParams[\"contamination\"])\n",
    "            LOFResults[LOFResults >= threshold] = 0\n",
    "            LOFResults[LOFResults < threshold] = -1\n",
    "        self.df[\"Cluster Assignment\"] = LOFResults\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'IF', 'epsilon': 1.0, 'minsamp': 2, 'estimators': 500, 'contamination': 0.1, 'neighbors': 5}\n",
      "{'method': 'Embedding', 'weighted': True, 'condense': False, 'n_topics': 5, 'aggregate_method': 'avg'}\n"
     ]
    }
   ],
   "source": [
    "dh = db.DatabaseHandler(\"testdb.db\")\n",
    "ad = AnomalyDetector(dh=dh, modelName=\"glove-wiki-gigaword-50\")\n",
    "ad.SetDefaultParams()\n",
    "ad.SetDFFromDB(eventID=19, splitBySentences=False)\n",
    "print(ad.AnomalyDetectionParams)\n",
    "print(ad.FeatureExtractionParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'method': 'Tiny',\n",
       " 'weighted': True,\n",
       " 'condense': False,\n",
       " 'n_topics': 5,\n",
       " 'aggregate_method': 'avg'}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad.SetDFFromDB(eventID=19, splitBySentences=False)\n",
    "ad.SetAnomalyDetectionParam(\"algorithm\", \"DBSCAN\")\n",
    "ad.SetAnomalyDetectionParam(\"neighbors\", 5)\n",
    "ad.SetFeatureExtractionParam(\"method\", \"Tiny\")\n",
    "ad.AnomalyDetectionParams\n",
    "ad.FeatureExtractionParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra-class functions\n",
    "\n",
    "def WriteInFile(df:pd.DataFrame, filePath:str=\"yakubian/ape.csv\"):\n",
    "    with open(file=filePath, mode=\"w\") as file:\n",
    "        file.write(\"question, answer\\n\")\n",
    "        for i in range(len(df.index)):\n",
    "            file.write(\"{}|{}\\n\".format(df.loc[i][\"question\"], df.loc[i][\"answer\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxdoc : ['opinion', 'sin', 'forgiven', 'catholicism', 'sin', 'forgiven', 'name', 'mortal', 'sin', 'bad', 'sin', 'god', 'forgive', 'especially', 'human', 'whoever', 'commits', 'mortal', 'sin', 'like', 'die', 'sin', 'forgiven', 'go', 'hell'], maxlen : 25\n",
      "Empty DataFrame\n",
      "Columns: [index, id, event_title, speaker, question, answer, Tokenized, Document Embed, Cluster Assignment]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# featureList = ad.df[\"Document Embed\"].tolist()\n",
    "# tokenList = ad.df[\"Tokenized\"].tolist()\n",
    "\n",
    "# for i in range(len(featureList)):\n",
    "#     print(len(featureList[i]), tokenList[i])\n",
    "outlier, good = ad.GetAnomalies(isReturnSeparate=True)\n",
    "\n",
    "# WriteInFile(outlier)\n",
    "\n",
    "print(outlier)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Gog, and Magog too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "EventIDList = [19, 20, 21, 25, 26] # add later\n",
    "FeatureExtractionList = [\"LDA\", \"Embedding\", \"Tiny\"]\n",
    "IsWeightedList = [True, False]\n",
    "AnomalyDetectionList = [\"DBSCAN\", \"LOF\", \"IF\"]\n",
    "LDATopicList = [5, 10, 20]\n",
    "WordEmbeddingModelList = [\"word2vec-google-news-300\",\n",
    "                          \"glove-wiki-gigaword-50\",\n",
    "                          \"glove-wiki-gigaword-100\",\n",
    "                          \"glove-wiki-gigaword-300\"]\n",
    "epsilonRange = np.arange(0.1, 2.1, 0.1)\n",
    "contaminatorRange = np.arange(0.1, 0.6, 0.1)\n",
    "\n",
    "AnomalyDetectionParamsDict ={\n",
    "    \"DBSCAN\" : {\n",
    "        \"epsilon\" : epsilonRange,\n",
    "        \"minsamp\" : [2,3,4,5],\n",
    "    },\n",
    "    \"LOF\":{\n",
    "        \"contaminator\" : contaminatorRange,\n",
    "        \"neighbors\" : [5, -5]\n",
    "    },\n",
    "    \"IF\":{\n",
    "        \"contaminator\": contaminatorRange,\n",
    "        \"estimator\" : [100, 500, 1000]\n",
    "    }\n",
    "}\n",
    "\n",
    "combinations = []\n",
    "\n",
    "for EventID in EventIDList:\n",
    "    for FeatureExtraction in FeatureExtractionList:\n",
    "        for IsWeighted in IsWeightedList:\n",
    "            for AnomalyDetection in AnomalyDetectionList:\n",
    "                if FeatureExtraction == \"LDA\":\n",
    "                    for LDATopic in LDATopicList:\n",
    "                        combination = {\n",
    "                            \"EventID\" : EventID,\n",
    "                            \"FeatureExtraction\" : FeatureExtraction,\n",
    "                            \"IsWeighted\" : IsWeighted,\n",
    "                            \"AnomalyDetection\" : AnomalyDetection,\n",
    "                            \"LDATopic\" : LDATopic\n",
    "                        }\n",
    "                        combinations.append(combination)\n",
    "                elif FeatureExtraction == \"Embedding\" or FeatureExtraction == \"Tiny\":\n",
    "                    for WordEmbeddingModel in WordEmbeddingModelList:\n",
    "                        combination = {\n",
    "                            \"EventID\" : EventID,\n",
    "                            \"FeatureExtraction\" : FeatureExtraction,\n",
    "                            \"IsWeighted\" : IsWeighted,\n",
    "                            \"AnomalyDetection\" : AnomalyDetection,\n",
    "                            \"Model\" : WordEmbeddingModel\n",
    "                        }\n",
    "                        combinations.append(combination)\n",
    "                \n",
    "\n",
    "# # Generate combinations for anomaly detection parameters\n",
    "# for combination in combinations:\n",
    "#     anomalyDetection = combination[\"AnomalyDetection\"]\n",
    "#     params = AnomalyDetectionParamsDict[anomalyDetection]\n",
    "#     parameter_combinations = list(product(*params.values()))\n",
    "#     for parameter_combination in parameter_combinations:\n",
    "#         parameter_values = dict(zip(params.keys(), parameter_combination))\n",
    "#         combination.update(parameter_values)\n",
    "\n",
    "# # for combo in combinations:\n",
    "# #     if \"epsilon\" in combo:\n",
    "# #         print(combo[\"epsilon\"])\n",
    "# for combo in combinations:\n",
    "#     print(combo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current model :  word2vec-google-news-300\n",
      "changed to :  word2vec-google-news-300\n",
      "current model :  glove-wiki-gigaword-50\n",
      "changed to :  glove-wiki-gigaword-50\n",
      "current model :  glove-wiki-gigaword-100\n",
      "changed to :  glove-wiki-gigaword-100\n",
      "current model :  glove-wiki-gigaword-300\n",
      "changed to :  glove-wiki-gigaword-300\n"
     ]
    }
   ],
   "source": [
    "EventIDList = [18, 19, 20, 21] # 25, 26] # add later\n",
    "FeatureExtractionList = [\"LDA\", \"Embedding\", \"Tiny\"]\n",
    "IsWeightedList = [True, False]\n",
    "AnomalyDetectionList = [\"DBSCAN\"] #, \"LOF\", \"IF\"]\n",
    "LDATopicList = [5, 10, 20]\n",
    "WordEmbeddingModelList = [\"word2vec-google-news-300\",\n",
    "                          \"glove-wiki-gigaword-50\",\n",
    "                          \"glove-wiki-gigaword-100\",\n",
    "                          \"glove-wiki-gigaword-300\"]\n",
    "epsilonRange = np.floor(np.arange(0.1, 2.1, 0.1) * 10)/10\n",
    "minsampRange = np.arange(2, 6)\n",
    "\n",
    "combinations = []\n",
    "for WordEmbeddingModel in WordEmbeddingModelList:\n",
    "    for EventID in EventIDList:\n",
    "        for FeatureExtraction in FeatureExtractionList:\n",
    "            for IsWeighted in IsWeightedList:\n",
    "                for AnomalyDetection in AnomalyDetectionList:\n",
    "                    for minsamp in minsampRange:\n",
    "                        for epsilon in epsilonRange:\n",
    "                            if FeatureExtraction == \"LDA\":\n",
    "                                for LDATopic in LDATopicList:\n",
    "                                    combination = {\n",
    "                                        \"EventID\" : EventID,\n",
    "                                        \"method\" : FeatureExtraction,\n",
    "                                        \"weighted\" : IsWeighted,\n",
    "                                        \"algorithm\" : AnomalyDetection,\n",
    "                                        \"n_topics\" : LDATopic,\n",
    "                                        \"epsilon\" : epsilon,\n",
    "                                        \"minsamp\" : minsamp\n",
    "                                    }\n",
    "                                    combinations.append(combination)\n",
    "                            elif FeatureExtraction == \"Embedding\" or FeatureExtraction == \"Tiny\":\n",
    "                                combination = {\n",
    "                                    \"EventID\" : EventID,\n",
    "                                    \"method\" : FeatureExtraction,\n",
    "                                    \"weighted\" : IsWeighted,\n",
    "                                    \"algorithm\" : AnomalyDetection,\n",
    "                                    \"Model\" : WordEmbeddingModel,\n",
    "                                    \"epsilon\" : epsilon,\n",
    "                                    \"minsamp\" : minsamp\n",
    "                                }\n",
    "                                combinations.append(combination)\n",
    "                                \n",
    "currentEventID = 0\n",
    "currentModel = \"\"\n",
    "\n",
    "for combo in combinations:\n",
    "    # set eventID\n",
    "    if combo[\"EventID\"] != currentEventID:\n",
    "        ad.SetDFFromDB(eventID=combo[\"EventID\"])\n",
    "        currentEventID = combo[\"EventID\"]\n",
    "    \n",
    "    # set model\n",
    "    if \"Model\" in combo and combo[\"Model\"] != currentModel:\n",
    "        print(\"current model : \", combo[\"Model\"])\n",
    "        ad.SetModel(combo[\"Model\"])\n",
    "        currentModel = combo[\"Model\"]\n",
    "        print(\"changed to : \", combo[\"Model\"])\n",
    "    \n",
    "    for key in ad.AnomalyDetectionParams.keys():\n",
    "        if key in combo:\n",
    "            ad.AnomalyDetectionParams[key] = combo[key]\n",
    "    for key in ad.FeatureExtractionParams.keys():\n",
    "        if key in combo:\n",
    "            ad.FeatureExtractionParams[key] = combo[key]\n",
    "\n",
    "    if \"Model\" in combo:\n",
    "        if combo[\"Model\"] == \"word2vec-google-news-300\":\n",
    "            modelShort = \"n300\"\n",
    "        elif combo[\"Model\"] == \"glove-wiki-gigaword-50\":\n",
    "            modelShort = \"g50\"\n",
    "        elif combo[\"Model\"] == \"glove-wiki-gigaword-100\":\n",
    "            modelShort = \"g100\"\n",
    "        elif combo[\"Model\"] == \"glove-wiki-gigaword-300\":\n",
    "            modelShort = \"g300\"\n",
    "    \n",
    "    if combo[\"method\"] == \"LDA\":\n",
    "        filePath = f'{combo[\"EventID\"]}_{ad.FeatureExtractionParams[\"method\"]}_{ad.FeatureExtractionParams[\"n_topics\"]}_{ad.FeatureExtractionParams[\"weighted\"]}_{ad.AnomalyDetectionParams[\"algorithm\"]}_{ad.AnomalyDetectionParams[\"epsilon\"]}_{ad.AnomalyDetectionParams[\"minsamp\"]}.csv'\n",
    "    elif combo[\"method\"] == \"Embedding\" or combo[\"method\"] == \"Tiny\":\n",
    "        filePath = f'{combo[\"EventID\"]}_{ad.FeatureExtractionParams[\"method\"]}_{combo[\"Model\"]}_{ad.FeatureExtractionParams[\"weighted\"]}_{ad.AnomalyDetectionParams[\"algorithm\"]}_{ad.AnomalyDetectionParams[\"epsilon\"]}_{ad.AnomalyDetectionParams[\"minsamp\"]}.csv'\n",
    "    outliers, goods = ad.GetAnomalies(isReturnSeparate=True)\n",
    "    WriteInFile(outliers, \"dbscan/{}\".format(filePath))\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
